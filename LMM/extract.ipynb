{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfminer\n",
    "from pdfminer.image import ImageWriter\n",
    "from pdfminer.high_level import extract_pages\n",
    "\n",
    "filename = \"/ai_dev/AI-Dojo/LMM/pdfs/idefics_paper.pdf\"\n",
    "# filename = \"/ai_dev/AI-Dojo/LMM/pdfs/pc_ho.pdf\"\n",
    "pages = list(extract_pages(filename))\n",
    "page = pages[19]\n",
    "\n",
    "\n",
    "def get_image(layout_object):\n",
    "    if isinstance(layout_object, pdfminer.layout.LTImage):\n",
    "        return layout_object\n",
    "    if isinstance(layout_object, pdfminer.layout.LTContainer):\n",
    "        for child in layout_object:\n",
    "            return get_image(child)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def save_images_from_page(page: pdfminer.layout.LTPage):\n",
    "    images = list(filter(bool, map(get_image, page)))\n",
    "    iw = ImageWriter('./extractions')\n",
    "    for image in images:\n",
    "        iw.export_image(image)\n",
    "\n",
    "save_images_from_page(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What matters when building vision-language models?\n",
      "\n",
      "Hugo Laurençon∗,1,2 Léo Tronchon∗,1 Matthieu Cord2 Victor Sanh1\n",
      "\n",
      "∗The order was chosen randomly.\n",
      "\n",
      "1Hugging Face 2Sorbonne Université\n",
      "\n",
      "Abstract\n",
      "\n",
      "The growing interest in vision-language models (VLMs) has been driven by im-\n",
      "provements in large language models and vision transformers. Despite the abun-\n",
      "dance of literature on this subject, we observe that critical decisions regarding the\n",
      "design of VLMs are often not justified. We argue that these unsupported decisions\n",
      "impede progress in the field by making it difficult to identify which choices im-\n",
      "prove model performance. To address this issue, we conduct extensive experiments\n",
      "around pre-trained models, architecture choice, data, and training methods. Our\n",
      "consolidation of findings includes the development of Idefics2, an efficient founda-\n",
      "tional VLM of 8 billion parameters. Idefics2 achieves state-of-the-art performance\n",
      "within its size category across various multimodal benchmarks, and is often on par\n",
      "with models four times its size. We release the model (base, instructed, and chat)\n",
      "along with the datasets created for its training.\n",
      "\n",
      "4\n",
      "2\n",
      "0\n",
      "2\n",
      "\n",
      "y\n",
      "a\n",
      "M\n",
      "3\n",
      "\n",
      "]\n",
      "\n",
      "V\n",
      "C\n",
      ".\n",
      "s\n",
      "c\n",
      "[\n",
      "\n",
      "1\n",
      "v\n",
      "6\n",
      "4\n",
      "2\n",
      "2\n",
      "0\n",
      ".\n",
      "5\n",
      "0\n",
      "4\n",
      "2\n",
      ":\n",
      "v\n",
      "i\n",
      "X\n",
      "r\n",
      "a\n",
      "\n",
      "Figure 1: Idefics2-chatty analyzes the table to compute and answer the query.\n",
      "\n",
      "Preprint. Under review.\n",
      "\n",
      "PromptFor 2024, the interest expense is twice what it was in 2014, and the long-term debt is 10% higher than its 2015 level. Can you calculate the combined total of the interest and long-term debt for 2024?Idefics2 outputYes, I can help you calculate that. First, let's find out the interest expense for 2024:Interest Expense (2024) = 2 * Interest Expense (2014)= 2 * 630= 1260Next, let's find the long-term debt for 2024:Long-Term Debt (2024) = Long-Term Debt (2015) + 10% of Long-Term Debt (2015)= 19,260 + 0.1 * 19,260= 19,260 + 1,926= 21,186Now, we can add these two amounts together to get the combined total of the interest and long-term debt for 2024:Combined Total (2024) = Interest Expense (2024) + Long-Term Debt (2024)= 1260 + 21,186= 22,446So, the combined total of the interest and long-term debt for 2024 is $22,446. \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\f1\n",
      "\n",
      "Introduction\n",
      "\n",
      "Vision-language models (VLMs) that take images and texts as inputs and output texts, are useful\n",
      "for many tasks, like retrieving information in a scanned PDF (Hu et al., 2024), explaining charts or\n",
      "diagrams (Carbune et al., 2024), transcribing the text in an image (Blecher et al., 2023), counting\n",
      "objects in a picture (Goyal et al., 2017) or turning screenshots of webpages into code (Laurençon\n",
      "et al., 2024). The development of powerful open large language models (Touvron et al., 2023; Jiang\n",
      "et al., 2023; Google, 2024b) and image encoders (Zhai et al., 2023; Sun et al., 2023; Radford et al.,\n",
      "2021) enables researchers to build upon these unimodal pre-trained models to create advanced VLMs\n",
      "that solve these problems with increasing accuracy (Dai et al., 2023; Liu et al., 2023; Bai et al.,\n",
      "2023; Lin et al., 2024, 2023; Li et al., 2024; Wang et al., 2024). Despite the progress in the field, the\n",
      "literature reveals many disparate design choices which are often not justified experimentally, or very\n",
      "briefly.\n",
      "\n",
      "This situation makes it challenging to distinguish which decisions truly account for model perfor-\n",
      "mance, thereby making it difficult for the community to make meaningful and grounded progress.\n",
      "For instance, (Alayrac et al., 2022; Laurençon et al., 2023) use interleaved Transformer-based cross-\n",
      "attentions to fuse the image information into the language model, while (Li et al., 2023; Liu et al.,\n",
      "2023) concatenate the sequence of image hidden states with the sequence of text embeddings, and\n",
      "feed the concatenated sequence to the language model. To our knowledge, this choice has not been\n",
      "properly ablated, and trade-offs in terms of compute, data efficiency and performance are poorly\n",
      "understood. In this work, we aim to bring experimental clarity to some of these core design choices\n",
      "and pose the question: What matters when building vision-language models?\n",
      "\n",
      "We identify two areas where various works adopt different design choices: (a) model architecture,\n",
      "and in particular, connector modules that fuse the vision and text modalities and their impact on\n",
      "inference efficiency, (b) multimodal training procedure and its impact on training stability. For each\n",
      "of these areas, we rigorously compare different design choices in a controlled environment and extract\n",
      "experimental findings. Notably, we find that (a) the progress of vision-language models is in large part\n",
      "driven by the progress of pre-trained unimodal backbones, (b) the more recent fully autoregressive\n",
      "architecture outperforms the cross-attention architecture, although it requires modifications to the\n",
      "optimization procedure to ensure a stable training, (c) adaptation of the pre-trained vision backbone\n",
      "and the modules connecting the text and vision modalities allow for more efficiency at inference\n",
      "time on one side, and handling images in their original ratio and size without harming downstream\n",
      "performance on the other side, and (d) modifications to the image processing enables trading inference\n",
      "cost for downstream performance.\n",
      "\n",
      "Our results are complementary with those presented in (Karamcheti et al., 2024; McKinzie et al.,\n",
      "2024; Lin et al., 2024) which derive insights about multi-stage training, selective unfreezing of\n",
      "the pre-trained backbones, data repetition, and impact of training mixture on zero and few-shot\n",
      "performance. We specifically delve into unexplored aspects such as model architecture, training\n",
      "methods, stability, and efficiency improvements at inference.\n",
      "\n",
      "Learning from these insights, we train Idefics2, a foundational VLM with 8 billion parameters.\n",
      "Idefics2 achieves state-of-the-art performance in its size category on various benchmarks while being\n",
      "more efficient at inference, for both the base and the fine-tuned version. It is on par with state-of-\n",
      "the-art models 4 times larger on some vision-language benchmarks and matches the performance of\n",
      "Gemini 1.5 Pro on some challenging benchmarks. We release the base, instructed, and chat versions\n",
      "of Idefics21 as resources for the VLM community along with the data created to train the model.\n",
      "\n",
      "2 Terminology\n",
      "\n",
      "We first establish shared terminology for discussing the different design choices. Training VLMs\n",
      "typically requires gluing together a pre-trained vision backbone and a pre-trained language backbone\n",
      "by initializing new parameters to connect the two modalities. Training these new parameters is\n",
      "done during the pre-training phase. This stage commonly leverages a large multimodal dataset such\n",
      "as image-caption pairs. We note that even though it is most common to start from two separate\n",
      "unimodal pre-trained backbones, the parameters of these two backbones can be optionally shared and\n",
      "initialized from scratch as done in (Bavishi et al., 2023). As in the large language models literature,\n",
      "\n",
      "1https://huggingface.co/collections/HuggingFaceM4/idefics2-661d1971b7c50831dd3ce0fe\n",
      "\n",
      "2\n",
      "\n",
      "\fthe pre-training stage is followed by an instruction fine-tuning stage, in which the model learns from\n",
      "task-oriented samples.\n",
      "\n",
      "Recent works explore two main choices to combine the visual inputs and the text inputs. In the\n",
      "cross-attention architecture (Alayrac et al., 2022; Laurençon et al., 2023; Awadalla et al., 2023),\n",
      "the images encoded through the vision backbone are injected at different layers within the language\n",
      "model by interleaving cross-attention blocks in which the text cross-attends to the image hidden states.\n",
      "In contrast, in the fully autoregressive architecture (Koh et al., 2023; Driess et al., 2023; Liu et al.,\n",
      "2023), the output of the vision encoder is directly concatenated to the sequence of text embeddings,\n",
      "and the entire sequence is passed as input to the language model. The input sequence of the language\n",
      "model is thus the concatenation of visual tokens and text tokens. The sequence of visual tokens can be\n",
      "optionally pooled into a shorter sequence, providing more compute efficiency. We refer to the layers\n",
      "that maps the vision hidden space to the text hidden space as modality projection layers. Figure 2\n",
      "highlights the fully-autoregressive architecture we ultimately use for Idefics2.\n",
      "\n",
      "Figure 2: Idefics2 fully-autoregressive architecture: Input images are processed by the Vision encoder.\n",
      "The resulting visual features are mapped (and optionally pooled) to the LLM input space to get the\n",
      "visual tokens (64 in our standard configuration). They are concatenated (and potentially interleaved)\n",
      "with the input sequence of text embeddings (green and red column). The concatenated sequence is\n",
      "fed to the language model (LLM ), which predicts the text tokens output.\n",
      "\n",
      "3 Exploring the design space of vision-language models\n",
      "\n",
      "In this section, we compare recurrent design choices in the vision-language model literature and\n",
      "highlight findings. Unless specified otherwise, we run the ablations for 6’000 steps and report\n",
      "the average score of the 4-shot performance on 4 downstream benchmarks measuring different\n",
      "capabilities: VQAv2 (Goyal et al., 2017) for general visual question answering, TextVQA (Singh\n",
      "et al., 2019) for OCR abilities, OKVQA (Marino et al., 2019) for external knowledge, and COCO\n",
      "(Lin et al., 2014) for captioning.\n",
      "\n",
      "3.1 Are all pre-trained backbones equivalent for VLMs?\n",
      "\n",
      "Most recent VLMs start from pre-trained unimodal backbones. How does the choice of the backbones\n",
      "(vision and text) influence the performance of the resulting VLM?\n",
      "\n",
      "We fix the size of the pretrained backbones, the data used for multi-\n",
      "modal pre-training, and the number of training updates. Under the\n",
      "cross-attention architecture, we observe that the greatest improve-\n",
      "ment in the performance on vision-language benchmarks comes\n",
      "from changing the language model to a better one. More specif-\n",
      "ically, replacing LLaMA-1-7B (Touvron et al., 2023) (35.1% on\n",
      "MMLU (Hendrycks et al., 2021)) by Mistral-7B (Jiang et al., 2023)\n",
      "(60.1% on MMLU) yields a boost of 5.1 (see Table 1). Additionally, switching the vision encoder\n",
      "from CLIP-ViT-H (Radford et al., 2021) (78.0% on ImageNet(Deng et al., 2009)) to SigLIP-SO400M\n",
      "\n",
      "Table 1: Ablation on the language\n",
      "model backbone.\n",
      "\n",
      "LM backbone Avg. score\n",
      "\n",
      "Llama-1-7B\n",
      "Mistral-7B\n",
      "\n",
      "62.5\n",
      "67.6\n",
      "\n",
      "3\n",
      "\n",
      "VisionencoderModality projection+PoolingLLMTTHidden statesUp to980x98064............64...64...64......\f(Zhai et al., 2023) (83.2% on ImageNet) yields a 3.3 increase in performance on the benchmarks (see\n",
      "Table 2). This result on better vision backbones corroborates observations from (Karamcheti et al.,\n",
      "2024).\n",
      "\n",
      "We note that Chen and Wang (2022) reports a stronger increase\n",
      "in performance by scaling the size of the vision encoder com-\n",
      "pared to scaling the size of the language model even though\n",
      "scaling the vision encoder leads to a smaller parameter count in-\n",
      "crease. Although EVA-CLIP-5B (Sun et al., 2023) is ten times\n",
      "bigger in parameter counts than SigLIP-SO400M (Zhai et al.,\n",
      "2023), we obtain similar performance across 4 benchmarks,\n",
      "suggesting that EVA-CLIP-5B could be heavily under-trained,\n",
      "and we acknowledge that the open VLM community is missing a large well-trained vision encoder.\n",
      "\n",
      "Table 2: Ablation on the vision encoder\n",
      "backbone.\n",
      "\n",
      "224\n",
      "CLIP-ViT-H\n",
      "EVA-CLIP-5B\n",
      "224\n",
      "SigLIP-SO400M 384\n",
      "\n",
      "Res. Avg. score\n",
      "\n",
      "VE backbone\n",
      "\n",
      "57.4\n",
      "60.2\n",
      "60.7\n",
      "\n",
      "Finding 1. For a fixed number of parameters, the quality of the language model backbone has\n",
      "a higher impact on the performance of the final VLM than the quality of the vision backbone.\n",
      "\n",
      "3.2 How does the fully autoregressive architecture compare to the cross-attention\n",
      "\n",
      "architecture?\n",
      "\n",
      "To our knowledge, there is no proper comparison between the fully autoregressive and the cross-\n",
      "attention architecture. We aim to fill this gap by considering their trade-offs, namely performance,\n",
      "parameter count, and inference cost.\n",
      "\n",
      "Avg. score\n",
      "\n",
      "Architecture\n",
      "\n",
      "Backbones\n",
      "training\n",
      "\n",
      "Following (Alayrac et al., 2022), we first com-\n",
      "pare the two architectures by freezing the\n",
      "unimodal backbones and training only the\n",
      "newly initialized parameters (cross-attention on\n",
      "one side, and modality projection along with\n",
      "learned pooling on the other side), while fix-\n",
      "ing the amount of training data. Alayrac et al.\n",
      "(2022) shows that the more frequently the cross-\n",
      "attention blocks are interleaved with the lan-\n",
      "guage model layers, and the higher the vision-language performance. As such, we note that under this\n",
      "setup, the cross-attention architecture has 1.3B more trainable parameters (2B trainable parameters in\n",
      "total) than the fully autoregressive architecture. Additionally, at inference time, the former uses 10%\n",
      "more flops than the latter. Under these conditions, we observe that the cross-attention architecture\n",
      "performs 7 points better in Table 3.\n",
      "\n",
      "Fully autoreg. no Perceiver\n",
      "Fully autoreg.\n",
      "Cross-attention\n",
      "Cross-attention\n",
      "Fully autoreg.\n",
      "\n",
      "Table 3: Ablation for the architecture and method of training.\n",
      "\n",
      "Frozen\n",
      "Frozen\n",
      "Frozen\n",
      "LoRA\n",
      "LoRA\n",
      "\n",
      "51.8\n",
      "60.3\n",
      "66.7\n",
      "67.3\n",
      "69.5\n",
      "\n",
      "Out of the total number of parameters, approximately 15% for the fully autoregressive architecture and\n",
      "25% for the cross-attention are trained. We hypothesize that this low proportion limits the expressivity\n",
      "of the training and hinders performance. To test that hypothesis, we compare the two architectures by\n",
      "unfreezing all parameters (newly initialized parameters and parameters of the pre-trained unimodal\n",
      "backbones). Under these conditions, training the fully autoregressive architecture would yield loss\n",
      "divergences, and we were not successful in stabilizing the training even by aggressively lowering\n",
      "the learning rate or gradually unfreezing various components. To overcome this stability challenge,\n",
      "we leverage Low-Rank Adaptation (Hu et al., 2022) to adapt the pre-trained parameters while using\n",
      "standard full fine-tuning for the newly initialized ones.\n",
      "\n",
      "This setup yields significantly more stable trainings, and more importantly, we observe a 12.9\n",
      "points increase under the fully autoregressive architecture, and 0.6 point under the cross-attention\n",
      "architecture. While the cross-attention architecture performs better than the fully autoregressive\n",
      "architecture with frozen backbones, it is worse when we add degrees of liberty for the pre-trained\n",
      "backbones. Besides, using LoRA allows training the unimodal backbones at a fraction of the GPU\n",
      "memory cost of full fine-tuning, and LoRA layers can be merged back into the original linear layers\n",
      "yielding no additional cost at inference. We therefore choose the fully autoregressive architecture in\n",
      "the rest of this work.\n",
      "\n",
      "It is interesting to note that this finding contradicts (Karamcheti et al., 2024) in which the authors\n",
      "observed that unfreezing the pre-trained visual backbone would significantly degrade the performance.\n",
      "We hypothesize that using parameter-efficient fine-tuning methods is a key difference.\n",
      "\n",
      "4\n",
      "\n",
      "\fFinding 2. The cross-attention architecture performs better than the fully autoregressive one\n",
      "when unimodal pre-trained backbones are kept frozen. However, when training the unimodal\n",
      "backbones, the fully autoregressive architecture outperforms the cross-attention one, even\n",
      "though the latter has more parameters.\n",
      "\n",
      "Finding 3. Unfreezing the pre-trained backbones under the fully autoregressive architecture\n",
      "can lead to training divergences. Leveraging LoRA still adds expressivity to the training and\n",
      "stabilizes it.\n",
      "\n",
      "3.3 Where are the efficiency gains?\n",
      "\n",
      "Number of visual tokens Recent VLMs typically route the entire sequence of the vision encoder’s\n",
      "hidden states directly into the modality projection layer, which subsequently inputs into the language\n",
      "model, without no pooling. This is motivated by previous works in which adding a pooling strategy,\n",
      "like average pooling, was found to deteriorate the performance (Vallaeys et al., 2024). This results\n",
      "in a high number of visual tokens for each image ranging from 576 for DeepSeek-VL (Lu et al.,\n",
      "2024) to 2890 for SPHINX-2k (Lin et al., 2023). With the resulting sequence lengths, training is\n",
      "computationally costly, and in-context learning with interleaved images and texts is challenging\n",
      "because it requires modifications to the language models to handle very large context windows.\n",
      "\n",
      "We reduce the sequence length of each image’s hidden states by using a perceiver resampler (Jaegle\n",
      "et al., 2021; Alayrac et al., 2022; Bai et al., 2023) as a form of trainable Transformer-based pooling.\n",
      "The number of queries (also referred to as latents) corresponds to the number of resulting visual\n",
      "tokens after the pooling. We observe that the learned pooling is effective in two ways: it increases the\n",
      "performance by 8.5 points on average and reduces the number of visual tokens necessary for each\n",
      "image from 729 to 64 (see Table 3).\n",
      "\n",
      "In contrast to (Vallaeys et al., 2024; McKinzie et al., 2024) which\n",
      "find that the more visual tokens the higher the performance, we\n",
      "observe no gains when using more than 64 visual tokens. We\n",
      "hypothesize that in a hypothetical scenario of infinite training\n",
      "on unlimited data, performance might eventually improve, at the\n",
      "cost of a longer training time. Other variations over the Perceiver\n",
      "architecture (Mañas et al., 2023; Darcet et al., 2024; Vallaeys\n",
      "et al., 2024) resulted in decreased performance.\n",
      "\n",
      "Pooling\n",
      "\n",
      "# vis. tok. Avg. score\n",
      "\n",
      "Perceiver\n",
      "Perceiver\n",
      "\n",
      "128\n",
      "64\n",
      "\n",
      "71.2\n",
      "71.7\n",
      "\n",
      "Table 4: Ablation on the pooling strat-\n",
      "egy.\n",
      "\n",
      "Finding 4. Reducing the number of visual tokens with learned pooling significantly improves\n",
      "compute efficiency at training and inference while improving performance on downstream\n",
      "tasks.\n",
      "\n",
      "Preserving the original aspect ratio and image resolution Vision encoders, such as SigLIP, are\n",
      "typically trained on fixed-size square images. Resizing images alters their original aspect ratio, which\n",
      "is problematic, for instance, for tasks requiring reading long texts. Furthermore, conditioning the\n",
      "training on a single resolution size inherently introduces limitations: a low resolution omits crucial\n",
      "visual details, while a high resolution leads to inefficiency in training and inference. Allowing the\n",
      "model to encode images at various resolutions allows users to decide how much compute is spent on\n",
      "each image.\n",
      "\n",
      "Following Lee et al. (2023); Dehghani et al. (2023), we pass\n",
      "the image patches to the vision encoder without resizing\n",
      "the image or modifying its aspect ratio. Given that SigLIP\n",
      "was trained on fixed-size low-resolution square images, we\n",
      "interpolate the pre-trained positional embeddings to allow for\n",
      "a higher resolution and train the vision encoder with LoRA\n",
      "parameters to adapt to these modifications.2 Our findings\n",
      "\n",
      "Images\n",
      "\n",
      "Res.\n",
      "\n",
      "Avg. score\n",
      "\n",
      "Square images\n",
      "AR preserving\n",
      "\n",
      "768\n",
      "378-768\n",
      "\n",
      "73.1\n",
      "72.1\n",
      "\n",
      "Table 5: Ablation on the aspect-ratio preserv-\n",
      "ing strategy.\n",
      "\n",
      "2Since SigLIP is trained with a fixed resolution, the positional embeddings can be interpreted both as absolute\n",
      "or relative positions. With the aspect ratio and resolution preserving, these positions become relative positional\n",
      "embeddings.\n",
      "\n",
      "5\n",
      "\n",
      "\findicate that the aspect ratio preserving strategy maintains performance levels on downstream tasks\n",
      "while unlocking computational flexibility during both training and inference (see Table 5).\n",
      "In\n",
      "particular, not having to resize images to the same high resolution allows for saving GPU memory\n",
      "and handling images at the resolution they require.\n",
      "\n",
      "Finding 5. Adapting a vision encoder pre-trained on fixed-size square images to preserve\n",
      "images’ original aspect ratio and resolution does not degrade performance while speeding up\n",
      "training and inference and reducing memory.\n",
      "\n",
      "3.4 How can one trade compute for performance?\n",
      "\n",
      "(Lin et al., 2023; Li et al., 2024; Liu et al., 2024; McKinzie et al., 2024) show that splitting an\n",
      "image into sub-images allows boosting the downstream performance with no changes to the model’s\n",
      "signature. An image is decomposed into sub-images (for instance 4 equal sub-images), which are\n",
      "then concatenated to the original image to form a sequence of 5 images. Additionally, the sub-images\n",
      "are resized to the original image’s size. This strategy however comes at the cost of a much higher\n",
      "number of tokens to encode the images.\n",
      "\n",
      "We adopt this strategy during the instruction fine-tuning stage. Each single image becomes a list\n",
      "of 5 images: 4 crops and the original image. This way, at inference, the model is able to deal with\n",
      "standalone images (64 visual tokens per image), as well as artificially augmented images (320 visual\n",
      "tokens in total per image). We notice that this strategy is particularly useful for benchmarks like\n",
      "TextVQA and DocVQA, which require a sufficiently high resolution to extract the text in an image\n",
      "(see Table 9).\n",
      "\n",
      "Moreover, when we apply image spitting to only 50% of the training samples (instead of 100% of the\n",
      "samples), we observe that it does not impair the performance increase that image splitting provides.\n",
      "Surprisingly, we find at evaluation time that increasing the resolution of the sub-images (and the\n",
      "standalone image) provides only a minor boost in performance compared to the improvement yielded\n",
      "by sole image splitting: 73.6% when increasing the resolution of the sub-images to the maximum\n",
      "vs 73.0% accuracy on our validation set of TextVQA, and respectively 72.7 vs 72.9 ANLS on the\n",
      "validation set of DocVQA.\n",
      "\n",
      "Finding 6. Splitting images into sub-images during training allow trading compute efficiency\n",
      "for more performance during inference. The increase in performance is particularly noticeable\n",
      "in tasks involving reading text in an image.\n",
      "\n",
      "4\n",
      "\n",
      "Idefics2 - an open state-of-the-art vision-language foundation model\n",
      "\n",
      "With these learnings in hand, we train an open 8B parameters vision-language model: Idefics2. This\n",
      "section describes the construction of the model, the choice of the dataset, the sequence of training\n",
      "phases and compares the resulting model against VLMs baselines.\n",
      "\n",
      "4.1 Multi-stage pre-training\n",
      "\n",
      "We start from SigLIP-SO400M and Mistral-7B-v0.1 and pre-train Idefics2 on 3 types of data.\n",
      "\n",
      "Interleaved image-text documents We use OBELICS (Laurençon et al., 2023), an open web-scale\n",
      "dataset of interleaved image-text documents with 350 million images and 115 billion text tokens. As\n",
      "shown by the authors, the long documents of OBELICS allow for preserving the performance of the\n",
      "language model while learning to deal with an arbitrary number of interleaved images and texts and\n",
      "long context. Additionally, the authors show that interleaved image-text documents are the biggest\n",
      "driving factor in increasing the performance on visual question answering (VQA) tasks, in particular\n",
      "in the in-context learning setup. We perform an additional removal of newly opted-out content in\n",
      "January 2024 using the Spawning API3 even though OBELICS had already been filtered to exclude\n",
      "opted-out content as of September 2023. We also removed the 5% of documents with the highest\n",
      "perplexity scores, as computed by Falcon-1B (Penedo et al., 2023).\n",
      "\n",
      "3https://spawning.ai/\n",
      "\n",
      "6\n",
      "\n",
      "\fCaptions Avg. score\n",
      "\n",
      "Image-text pairs Training on image-text pairs allows the model to\n",
      "learn the alignment between images and their associated texts. We use\n",
      "a combination of high-quality human-annotated image-text pairs from\n",
      "PMD (Singh et al., 2022) and higher-noise web-scale image-text pairs\n",
      "from (Schuhmann et al., 2022). To limit the amount of poor-quality data,\n",
      "we opt for the synthetic captions obtained through the LAION COCO4\n",
      "version of the dataset where images have been captioned with a model\n",
      "trained on COCO. This improves the quality of the training samples and thus the quality of the\n",
      "resulting model (see Table 6). We use a NSFW classifier5 with a high recall and remove 7% of the\n",
      "samples in LAION COCO. We manually inspect 5’000 examples and found 28 pornographic images\n",
      "in the original LAION COCO and only 1 after filtering. This filtering does not negatively impact the\n",
      "downstream performance.\n",
      "\n",
      "Table 6: Ablation on synthetic\n",
      "captions against alt-text for\n",
      "image-text pairs.\n",
      "\n",
      "Alt-texts\n",
      "Synthetic\n",
      "\n",
      "49.8\n",
      "52.9\n",
      "\n",
      "OCR data Res. DocVQA\n",
      "\n",
      "PDF documents Sun et al. (2023) shows that a large pro-\n",
      "portion of mistakes of state-of-the art VLMs stem from their\n",
      "failure to accurately extract text in images or documents. In\n",
      "order to obtain strong OCR and document understanding abil-\n",
      "ities, we train Idefics2 on different sources of PDF documents:\n",
      "19 million industry documents from OCR-IDL (Biten et al.,\n",
      "2022) and 18 million pages from PDFA6. Moreover, we add\n",
      "Rendered Text7 to complement the dataset with texts writ-\n",
      "ten with a wide variety of fonts and colors and on diverse\n",
      "backgrounds. These integrations significantly boost the per-\n",
      "formance on benchmarks that require reading text without decreasing the performance on other\n",
      "benchmarks (see Table 7).\n",
      "\n",
      "Table 7: Ablation on the synergy be-\n",
      "tween OCR data and image resolution.\n",
      "We pre-trained the models for 5’500\n",
      "steps, followed by 500 steps of fine-\n",
      "tuning on DocVQA.\n",
      "\n",
      "22.6\n",
      "42.9\n",
      "49.9\n",
      "\n",
      "W/o\n",
      "W/o\n",
      "W/\n",
      "\n",
      "384\n",
      "768\n",
      "768\n",
      "\n",
      "To maximize compute efficiency, we decompose the pre-training in two stages. In the first stage, we\n",
      "limit the max image resolution to 384 pixels, which allows us to use a large global batch size of 2’048\n",
      "(17k images and 2.5M text tokens on average). We sample OBELICS for 70% of the examples with a\n",
      "maximum sequence length of 2’048, and the image-text pairs datasets for 30% of the examples with\n",
      "a maximum sequence length of 1’536. In the second stage, we introduce PDF documents. Since they\n",
      "require a higher image resolution for the text to be legible, we increase the resolution to a maximum\n",
      "of 980 pixels. We use the same global batch size, but have to decrease the per-device batch size and\n",
      "use gradient accumulation to compensate for the additional memory cost. OBELICS represents 45%\n",
      "of the examples with a maximum sequence length of 2’048, image-text pairs represent 35% of the\n",
      "examples with a maximum sequence length of 1’536, and PDF documents represent the remaining\n",
      "20% of the examples with a maximum sequence length of 1’024. Additionally, we randomly scale\n",
      "up images to adequately cover the distribution of potential image sizes. We emphasize that the\n",
      "training stages are different than the ones ablated in (Karamcheti et al., 2024): instead of selectively\n",
      "freezing/unfreezing parts of the model, we train the entire model during both stages (some parameters\n",
      "are trained with LoRA) and increase the image resolution from one stage to the other.\n",
      "We use a learning rate of 10−4 and do around 2 epochs on our training data. It corresponds to\n",
      "approximately 1.5 billion images and 225 billion text tokens. We note that this is orders of magnitude\n",
      "more training data than other open VLMs. For example, ShareGPT (Chen et al., 2023) uses 1.2\n",
      "million images, while Monkey (Li et al., 2024) uses 1.4 million for training.\n",
      "\n",
      "To evaluate the base model, we consider VQAv2 (Goyal et al., 2017), TextVQA (Singh et al., 2019),\n",
      "OKVQA (Marino et al., 2019), and COCO (Lin et al., 2014). Table 8 presents the results. While\n",
      "having fewer tokens per image, and thus being more efficient, Idefics2 performs favorably compared\n",
      "to the other current best base VLMs (OpenFlamingo (Awadalla et al., 2023), Idefics1 (Laurençon\n",
      "et al., 2023), Flamingo (Alayrac et al., 2022), and MM1 (McKinzie et al., 2024)). It is notably much\n",
      "better at reading texts in an image. Figure 3 shows an example of an output from the base model on a\n",
      "task similar to the pre-training.\n",
      "\n",
      "4https://laion.ai/blog/laion-coco/\n",
      "5https://github.com/LAION-AI/LAION-SAFETY\n",
      "6https://huggingface.co/datasets/pixparse/pdfa-eng-wds\n",
      "7https://huggingface.co/datasets/wendlerc/RenderedText\n",
      "\n",
      "7\n",
      "\n",
      "\fModel\n",
      "\n",
      "Size Archi.\n",
      "\n",
      "# tokens\n",
      "per image\n",
      "\n",
      "VQAv2 TextVQA OKVQA COCO\n",
      "\n",
      "OpenFlamingo\n",
      "Idefics1\n",
      "Flamingo\n",
      "MM1\n",
      "\n",
      "Idefics2-base\n",
      "\n",
      "9B\n",
      "9B\n",
      "9B\n",
      "7B\n",
      "\n",
      "8B\n",
      "\n",
      "CA\n",
      "CA\n",
      "CA\n",
      "FA\n",
      "\n",
      "FA\n",
      "\n",
      "-\n",
      "-\n",
      "-\n",
      "144\n",
      "\n",
      "64\n",
      "\n",
      "54.8\n",
      "56.4\n",
      "58.0\n",
      "63.6\n",
      "\n",
      "70.3\n",
      "\n",
      "29.1\n",
      "27.5\n",
      "33.6\n",
      "46.3\n",
      "\n",
      "57.9\n",
      "\n",
      "41.1\n",
      "47.7\n",
      "50.0\n",
      "51.4\n",
      "\n",
      "54.6\n",
      "\n",
      "96.3\n",
      "97.0\n",
      "99.0\n",
      "116.3\n",
      "\n",
      "116.0\n",
      "\n",
      "Table 8: Performance of Idefics2-base against state-of-the-art base VLMs. The evaluations were done\n",
      "with 8 random in-context examples, and in an open-ended setting for VQA tasks.\n",
      "FA: fully autoregressive architecture. CA: cross-attention architecture.\n",
      "(Task, Metric, Split): (VQAv2, VQA acc., testdev), (TextVQA, VQA acc., val), (OKVQA, VQA acc.,\n",
      "val), (COCO, CIDEr, test)\n",
      "\n",
      "Figure 3: An example of text transcription with Idefics2-base.\n",
      "\n",
      "4.2\n",
      "\n",
      "Instruction fine-tuning\n",
      "\n",
      "We continue the training with an instruction fine-tuning phase.\n",
      "To do so, we create and release The Cauldron8, a massive collection of 50 vision-language datasets,\n",
      "covering a wide range of tasks: general visual question answering, counting, captioning, text tran-\n",
      "scription, document understanding, chart/figure understanding, table understanding, visual reasoning,\n",
      "geometry, spotting differences between 2 images or converting a screenshot to a functional code. Sim-\n",
      "ilarly to (Sanh et al., 2022; Wei et al., 2022; Bach et al., 2022; Dai et al., 2023; Li et al., 2023), each\n",
      "dataset is prompted into a shared question/answer format. When there are multiple question/answer\n",
      "pairs per image, we concatenate the pairs into a multi-turn conversation. We deduplicate the training\n",
      "set against the evaluation sets, ensuring that there is minimum contamination from the training to the\n",
      "evaluation.\n",
      "\n",
      "In addition to these vision-language datasets and following insights from (McKinzie et al., 2024), we\n",
      "add text-only instruction datasets to the mixture. The datasets aim at teaching the model to follow\n",
      "complex instructions, solve mathematical problems, or do arithmetic calculations. We give more\n",
      "details about the chosen datasets, the number of images, question-answer pairs, and size of each of\n",
      "the subsets, as well as our selected mixture proportion in Table 14 in Appendix A.2.1.\n",
      "\n",
      "We instruction-tune the base model using DoRA (Liu et al., 2024) (a variant of LoRA). During the\n",
      "fine-tuning, we only compute the loss on the tokens of the answers in the Q/A pairs. Since we are\n",
      "doing many epochs over some of the datasets, we employ several strategies to lower the risk of\n",
      "overfitting. First, we add noise to the embeddings with the NEFTune (Jain et al., 2024) technique.\n",
      "Then, we scale up randomly the resolution of the images during the training. Finally, when applicable,\n",
      "we shuffle the multiple user/assistant turns randomly before feeding the example to the model.\n",
      "\n",
      "We evaluate Idefics2 on commonly adopted benchmarks: MMMU (Yue et al., 2024) for multidis-\n",
      "cipline college-level problems, MathVista (Lu et al., 2024) for mathematical reasoning, TextVQA\n",
      "\n",
      "8https://huggingface.co/datasets/HuggingFaceM4/the_cauldron\n",
      "\n",
      "8\n",
      "\n",
      "Prompt:<image>The content of the letter is:\\nCompletion:Dear Craig,Actually, I've never been big on letter writing, though it is nice to receive a hand written note from someone. I am afraid that the art of drawing, and the related ability to sketch an idea, are also being lost. But perhaps, as the cliche goes, what goes around comes around.Good luck with your project.Tom Cianfichi\fModel\n",
      "\n",
      "LLaVA-NeXT\n",
      "DeepSeek-VL\n",
      "MM1-Chat\n",
      "\n",
      "Idefics2\n",
      "Idefics2\n",
      "\n",
      "Size\n",
      "\n",
      "13B\n",
      "7B\n",
      "7B\n",
      "\n",
      "8B\n",
      "8B\n",
      "\n",
      "# tokens\n",
      "per image\n",
      "\n",
      "MMMU MathVista TextVQA MMBench\n",
      "\n",
      "2880\n",
      "576\n",
      "720\n",
      "\n",
      "64\n",
      "320\n",
      "\n",
      "36.2/-\n",
      "36.6/-\n",
      "37.0/35.6\n",
      "\n",
      "43.5/37.9\n",
      "43.0/37.7\n",
      "\n",
      "35.3\n",
      "36.1\n",
      "35.9\n",
      "\n",
      "51.6\n",
      "51.4\n",
      "\n",
      "67.1\n",
      "64.4\n",
      "72.8\n",
      "\n",
      "70.4\n",
      "73.0\n",
      "\n",
      "70.0\n",
      "73.2\n",
      "72.3\n",
      "\n",
      "76.8\n",
      "76.7\n",
      "\n",
      "Table 9: Performance of Idefics2 against state-of-the-art VLMs up to a size of 14B parameters. The\n",
      "evaluations are done in zero shot. Idefics2 with 64 or 320 tokens per image is the same model (same\n",
      "weights), only the inference differs. The full table is present in Appendix A.3.2.\n",
      "(Benchmark, Split, Metric): (MMMU, val/test, MMMU score), (MathVista, testmini, MMMU score),\n",
      "(TextVQA, val, VQA acc.), (MMBench, test, accuracy).\n",
      "\n",
      "(Singh et al., 2019) for text reading on natural images, and MMBench Liu et al. (2023) for various\n",
      "perception and reasoning tasks. Table 9 presents the results (see Table 15 for the complete result\n",
      "table) of Idefics2 against the current strongest VLMs in its class size: LLaVA-Next (Liu et al., 2024),\n",
      "DeepSeek-VL (Lu et al., 2024) and MM1-Chat (McKinzie et al., 2024). While being computation-\n",
      "ally much more efficient at inference, Idefics2 exhibits strong performance on various benchmarks,\n",
      "outperforming the current best foundation VLMs in its size category. It is on par with state-of-the-art\n",
      "models 4x its size, or with closed-source models like Gemini 1.5 Pro on several benchmarks like\n",
      "MathVista or TextVQA.\n",
      "\n",
      "4.3 Optimizing for chat scenarios\n",
      "\n",
      "The evaluation benchmarks expect very short answers, but humans prefer long generations when inter-\n",
      "acting with a model. We find that Idefics2 can exhibit difficulties in precisely following instructions\n",
      "about the expected format, making it difficult to reconcile “chattiness“ and downstream performance.\n",
      "As such, after instruction fine-tuning, we further train Idefics2 on dialogue data. We fine-tune Idefics2\n",
      "for a few hundred steps on LLaVA-Conv (Liu et al., 2023) and ShareGPT4V (Chen et al., 2023),\n",
      "with a large batch size. Our blind human evaluations reveal that Idefics2-chatty is overwhelmingly\n",
      "preferred over its instruction fine-tuned version in many user interactions. We also adversarially\n",
      "stress-tested the model to generate inaccurate, biased, or offensive responses and reported the findings\n",
      "in Appendix A.4. We show examples of generations with Idefics2-chatty in Figure 1, and in Appendix\n",
      "in Figures 5, 6 and 7.\n",
      "\n",
      "5 Conclusion\n",
      "\n",
      "In this work, we re-examine common choices made in the VLM literature and rigorously compare\n",
      "these choices in controlled experiments. Our findings touch upon the effectiveness of different\n",
      "architectures, their performance/inference cost trade-offs as well as training stability. With these\n",
      "learnings at hand, we train Idefics2, an open 8B parameters vision-language model. Idefics2 is\n",
      "state-of-the-art on various benchmarks in its category size and is much more efficient at inference.\n",
      "By releasing our findings, as well as our models and our training dataset, we aim to contribute to the\n",
      "ongoing evolution of VLMs and their applications in solving complex real-world problems.\n",
      "\n",
      "9\n",
      "\n",
      "\fAcknowledgement\n",
      "\n",
      "We thank Mustafa Shukor for helpful suggestions on the paper, and Yacine Jernite, Sasha Luccioni,\n",
      "Margaret Mitchell, Giada Pistilli, Lucie-Aimée Kaffee, and Jack Kumar for red-teaming the model.\n",
      "\n",
      "References\n",
      "\n",
      "Acharya, M., K. Kafle, and C. Kanan (2019). Tallyqa: Answering complex counting questions. In\n",
      "\n",
      "AAAI.\n",
      "\n",
      "Agrawal, H., K. Desai, Y. Wang, X. Chen, R. Jain, M. Johnson, D. Batra, D. Parikh, S. Lee, and\n",
      "In 2019 IEEE/CVF\n",
      "\n",
      "P. Anderson (2019, October). nocaps: novel object captioning at scale.\n",
      "International Conference on Computer Vision (ICCV). IEEE.\n",
      "\n",
      "Alayrac, J.-B., J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican,\n",
      "M. Reynolds, R. Ring, E. Rutherford, S. Cabi, T. Han, Z. Gong, S. Samangooei, M. Monteiro, J. L.\n",
      "Menick, S. Borgeaud, A. Brock, A. Nematzadeh, S. Sharifzadeh, M. a. Bi´nkowski, R. Barreira,\n",
      "O. Vinyals, A. Zisserman, and K. Simonyan (2022). Flamingo: a visual language model for\n",
      "few-shot learning. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh\n",
      "(Eds.), Advances in Neural Information Processing Systems, Volume 35, pp. 23716–23736. Curran\n",
      "Associates, Inc.\n",
      "\n",
      "Antol, S., A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick, and D. Parikh (2015). VQA:\n",
      "\n",
      "Visual Question Answering. In International Conference on Computer Vision (ICCV).\n",
      "\n",
      "Awadalla, A., I. Gao, J. Gardner, J. Hessel, Y. Hanafy, W. Zhu, K. Marathe, Y. Bitton, S. Gadre,\n",
      "S. Sagawa, J. Jitsev, S. Kornblith, P. W. Koh, G. Ilharco, M. Wortsman, and L. Schmidt (2023).\n",
      "Openflamingo: An open-source framework for training large autoregressive vision-language\n",
      "models.\n",
      "\n",
      "Bach, S., V. Sanh, Z. X. Yong, A. Webson, C. Raffel, N. V. Nayak, A. Sharma, T. Kim, M. S. Bari,\n",
      "T. Fevry, Z. Alyafeai, M. Dey, A. Santilli, Z. Sun, S. Ben-david, C. Xu, G. Chhablani, H. Wang,\n",
      "J. Fries, M. Al-shaibani, S. Sharma, U. Thakker, K. Almubarak, X. Tang, D. Radev, M. T.-j. Jiang,\n",
      "and A. Rush (2022, May). PromptSource: An integrated development environment and repository\n",
      "for natural language prompts. In V. Basile, Z. Kozareva, and S. Stajner (Eds.), Proceedings of the\n",
      "60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations,\n",
      "Dublin, Ireland, pp. 93–104. Association for Computational Linguistics.\n",
      "\n",
      "Bai, J., S. Bai, S. Yang, S. Wang, S. Tan, P. Wang, J. Lin, C. Zhou, and J. Zhou (2023). Qwen-vl: A\n",
      "\n",
      "versatile vision-language model for understanding, localization, text reading, and beyond.\n",
      "\n",
      "Bavishi, R., E. Elsen, C. Hawthorne, M. Nye, A. Odena, A. Somani, and S. Ta¸sırlar (2023). Introduc-\n",
      "\n",
      "ing our multimodal models.\n",
      "\n",
      "Belouadi, J., A. Lauscher, and S. Eger (2024). Automatikz: Text-guided synthesis of scientific vector\n",
      "\n",
      "graphics with tikz.\n",
      "\n",
      "Biten, A. F., R. Tito, L. Gomez, E. Valveny, and D. Karatzas (2022). Ocr-idl: Ocr annotations for\n",
      "\n",
      "industry document library dataset.\n",
      "\n",
      "Biten, A. F., R. Tito, A. Mafla, L. Gomez, M. Rusiñol, C. Jawahar, E. Valveny, and D. Karatzas\n",
      "(2019). Scene text visual question answering. In 2019 IEEE/CVF International Conference on\n",
      "Computer Vision (ICCV), pp. 4290–4300.\n",
      "\n",
      "Blecher, L., G. Cucurull, T. Scialom, and R. Stojnic (2023). Nougat: Neural optical understanding\n",
      "\n",
      "for academic documents.\n",
      "\n",
      "Brown, T., B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,\n",
      "G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh,\n",
      "D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess,\n",
      "J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei (2020). Language\n",
      "models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin\n",
      "(Eds.), Advances in Neural Information Processing Systems, Volume 33, pp. 1877–1901. Curran\n",
      "Associates, Inc.\n",
      "\n",
      "10\n",
      "\n",
      "\fCarbune, V., H. Mansoor, F. Liu, R. Aralikatte, G. Baechler, J. Chen, and A. Sharma (2024). Chart-\n",
      "\n",
      "based reasoning: Transferring capabilities from llms to vlms.\n",
      "\n",
      "Chang, S., D. Palzer, J. Li, E. Fosler-Lussier, and N. Xiao (2022). MapQA: A dataset for question\n",
      "\n",
      "answering on choropleth maps. In NeurIPS 2022 First Table Representation Workshop.\n",
      "\n",
      "Changpinyo, S., P. Sharma, N. Ding, and R. Soricut (2021). Conceptual 12M: Pushing web-scale\n",
      "\n",
      "image-text pre-training to recognize long-tail visual concepts. In CVPR.\n",
      "\n",
      "Chen, L., J. Li, X. Dong, P. Zhang, C. He, J. Wang, F. Zhao, and D. Lin (2023). Sharegpt4v:\n",
      "\n",
      "Improving large multi-modal models with better captions.\n",
      "\n",
      "Chen, X., J. Djolonga, P. Padlewski, B. Mustafa, S. Changpinyo, J. Wu, C. R. Ruiz, S. Goodman,\n",
      "X. Wang, Y. Tay, S. Shakeri, M. Dehghani, D. Salz, M. Lucic, M. Tschannen, A. Nagrani, H. Hu,\n",
      "M. Joshi, B. Pang, C. Montgomery, P. Pietrzyk, M. Ritter, A. Piergiovanni, M. Minderer, F. Pavetic,\n",
      "A. Waters, G. Li, I. Alabdulmohsin, L. Beyer, J. Amelot, K. Lee, A. P. Steiner, Y. Li, D. Keysers,\n",
      "A. Arnab, Y. Xu, K. Rong, A. Kolesnikov, M. Seyedhosseini, A. Angelova, X. Zhai, N. Houlsby,\n",
      "and R. Soricut (2023). Pali-x: On scaling up a multilingual vision and language model.\n",
      "\n",
      "Chen, X. and X. Wang (2022). Pali: Scaling language-image learning in 100+ languages.\n",
      "\n",
      "In\n",
      "\n",
      "Conference on Neural Information Processing Systems (NeurIPS).\n",
      "\n",
      "Chen, X., X. Wang, L. Beyer, A. Kolesnikov, J. Wu, P. Voigtlaender, B. Mustafa, S. Goodman,\n",
      "I. Alabdulmohsin, P. Padlewski, D. Salz, X. Xiong, D. Vlasic, F. Pavetic, K. Rong, T. Yu, D. Keysers,\n",
      "X. Zhai, and R. Soricut (2023). Pali-3 vision language models: Smaller, faster, stronger.\n",
      "\n",
      "Chen, Z., W. Chen, C. Smiley, S. Shah, I. Borova, D. Langdon, R. Moussa, M. Beane, T.-H. Huang,\n",
      "B. Routledge, and W. Y. Wang (2021, November). FinQA: A dataset of numerical reasoning over\n",
      "financial data. In M.-F. Moens, X. Huang, L. Specia, and S. W.-t. Yih (Eds.), Proceedings of the\n",
      "2021 Conference on Empirical Methods in Natural Language Processing, Online and Punta Cana,\n",
      "Dominican Republic, pp. 3697–3711. Association for Computational Linguistics.\n",
      "\n",
      "Cheng, Z., H. Dong, Z. Wang, R. Jia, J. Guo, Y. Gao, S. Han, J.-G. Lou, and D. Zhang (2022, May).\n",
      "HiTab: A hierarchical table dataset for question answering and natural language generation. In\n",
      "S. Muresan, P. Nakov, and A. Villavicencio (Eds.), Proceedings of the 60th Annual Meeting of\n",
      "the Association for Computational Linguistics (Volume 1: Long Papers), Dublin, Ireland, pp.\n",
      "1094–1110. Association for Computational Linguistics.\n",
      "\n",
      "Chu, X., L. Qiao, X. Zhang, S. Xu, F. Wei, Y. Yang, X. Sun, Y. Hu, X. Lin, B. Zhang, and C. Shen\n",
      "\n",
      "(2024). Mobilevlm v2: Faster and stronger baseline for vision language model.\n",
      "\n",
      "Conover, M., M. Hayes, A. Mathur,\n",
      "\n",
      "J. Wan, S. Shah, A. Ghodsi, P. Wen-\n",
      "Introducing the world’s first\n",
      "dell, M. Zaharia, and R. Xin (2023).\n",
      "truly open instruction-tuned llm.\n",
      "https://www.databricks.com/blog/2023/04/12/\n",
      "dolly-first-open-commercially-viable-instruction-tuned-llm. Accessed: 2023-\n",
      "06-30.\n",
      "\n",
      "Free dolly:\n",
      "\n",
      "J. Xie,\n",
      "\n",
      "Dai, W., J. Li, D. Li, A. Tiong, J. Zhao, W. Wang, B. Li, P. Fung, and S. Hoi (2023). InstructBLIP:\n",
      "In Thirty-seventh\n",
      "\n",
      "Towards general-purpose vision-language models with instruction tuning.\n",
      "Conference on Neural Information Processing Systems.\n",
      "\n",
      "Darcet, T., M. Oquab, J. Mairal, and P. Bojanowski (2024). Vision transformers need registers. In\n",
      "\n",
      "The Twelfth International Conference on Learning Representations.\n",
      "\n",
      "Dehghani, M., J. Djolonga, B. Mustafa, P. Padlewski, J. Heek, J. Gilmer, A. P. Steiner, M. Caron,\n",
      "R. Geirhos, I. Alabdulmohsin, R. Jenatton, L. Beyer, M. Tschannen, A. Arnab, X. Wang,\n",
      "C. Riquelme Ruiz, M. Minderer, J. Puigcerver, U. Evci, M. Kumar, S. V. Steenkiste, G. F. Elsayed,\n",
      "A. Mahendran, F. Yu, A. Oliver, F. Huot, J. Bastings, M. Collier, A. A. Gritsenko, V. Birodkar, C. N.\n",
      "Vasconcelos, Y. Tay, T. Mensink, A. Kolesnikov, F. Pavetic, D. Tran, T. Kipf, M. Lucic, X. Zhai,\n",
      "D. Keysers, J. J. Harmsen, and N. Houlsby (2023, 23–29 Jul). Scaling vision transformers to 22\n",
      "billion parameters. In A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett\n",
      "(Eds.), Proceedings of the 40th International Conference on Machine Learning, Volume 202 of\n",
      "Proceedings of Machine Learning Research, pp. 7480–7512. PMLR.\n",
      "\n",
      "11\n",
      "\n",
      "\fDehghani, M., B. Mustafa, J. Djolonga, J. Heek, M. Minderer, M. Caron, A. P. Steiner, J. Puigcerver,\n",
      "R. Geirhos, I. Alabdulmohsin, A. Oliver, P. Padlewski, A. A. Gritsenko, M. Lucic, and N. Houlsby\n",
      "(2023). Patch n’ pack: Navit, a vision transformer for any aspect ratio and resolution. In Thirty-\n",
      "seventh Conference on Neural Information Processing Systems.\n",
      "\n",
      "Deng, J., W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei (2009). Imagenet: A large-scale hierar-\n",
      "chical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pp.\n",
      "248–255.\n",
      "\n",
      "Desai, K., G. Kaul, Z. Aysola, and J. Johnson (2021). Redcaps: Web-curated image-text data created\n",
      "by the people, for the people. In J. Vanschoren and S. Yeung (Eds.), Proceedings of the Neural\n",
      "Information Processing Systems Track on Datasets and Benchmarks, Volume 1. Curran.\n",
      "\n",
      "Driess, D., F. Xia, M. S. M. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A. Wahid, J. Tompson,\n",
      "Q. Vuong, T. Yu, W. Huang, Y. Chebotar, P. Sermanet, D. Duckworth, S. Levine, V. Vanhoucke,\n",
      "K. Hausman, M. Toussaint, K. Greff, A. Zeng, I. Mordatch, and P. Florence (2023). Palm-e: an\n",
      "embodied multimodal language model. In Proceedings of the 40th International Conference on\n",
      "Machine Learning, ICML’23. JMLR.org.\n",
      "\n",
      "Gao, P., R. Zhang, C. Liu, L. Qiu, S. Huang, W. Lin, S. Zhao, S. Geng, Z. Lin, P. Jin, K. Zhang,\n",
      "W. Shao, C. Xu, C. He, J. He, H. Shao, P. Lu, H. Li, and Y. Qiao (2024). Sphinx-x: Scaling data\n",
      "and parameters for a family of multi-modal large language models.\n",
      "\n",
      "Google (2023). Gemini: A family of highly capable multimodal models.\n",
      "\n",
      "Google (2024a). Gemini 1.5: Unlocking multimodal understanding across millions of tokens of\n",
      "\n",
      "context.\n",
      "\n",
      "Google (2024b). Gemma: Open models based on gemini research and technology.\n",
      "\n",
      "Goyal, Y., T. Khot, D. Summers-Stay, D. Batra, and D. Parikh (2017). Making the v in vqa matter:\n",
      "Elevating the role of image understanding in visual question answering. In 2017 IEEE Conference\n",
      "on Computer Vision and Pattern Recognition (CVPR), pp. 6325–6334.\n",
      "\n",
      "He, X., Y. Zhang, L. Mou, E. Xing, and P. Xie (2020). Pathvqa: 30000+ questions for medical visual\n",
      "\n",
      "question answering.\n",
      "\n",
      "Hendrycks, D., C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt (2021).\n",
      "Measuring massive multitask language understanding. In International Conference on Learning\n",
      "Representations.\n",
      "\n",
      "Hong, W., W. Wang, Q. Lv, J. Xu, W. Yu, J. Ji, Y. Wang, Z. Wang, Y. Zhang, J. Li, B. Xu, Y. Dong,\n",
      "\n",
      "M. Ding, and J. Tang (2023). Cogagent: A visual language model for gui agents.\n",
      "\n",
      "Hu, A., H. Xu, J. Ye, M. Yan, L. Zhang, B. Zhang, C. Li, J. Zhang, Q. Jin, F. Huang, and J. Zhou\n",
      "\n",
      "(2024). mplug-docowl 1.5: Unified structure learning for ocr-free document understanding.\n",
      "\n",
      "Hu, E. J., yelong shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen (2022).\n",
      "LoRA: Low-rank adaptation of large language models. In International Conference on Learning\n",
      "Representations.\n",
      "\n",
      "Huang, S., L. Dong, W. Wang, Y. Hao, S. Singhal, S. Ma, T. Lv, L. Cui, O. K. Mohammed, B. Patra,\n",
      "Q. Liu, K. Aggarwal, Z. Chi, J. Bjorck, V. Chaudhary, S. Som, X. Song, and F. Wei (2023).\n",
      "Language is not all you need: Aligning perception with language models. In Thirty-seventh\n",
      "Conference on Neural Information Processing Systems.\n",
      "\n",
      "Hudson, D. A. and C. D. Manning (2019). Gqa: A new dataset for real-world visual reasoning\n",
      "and compositional question answering. In 2019 IEEE/CVF Conference on Computer Vision and\n",
      "Pattern Recognition (CVPR), pp. 6693–6702.\n",
      "\n",
      "Iyyer, M., W.-t. Yih, and M.-W. Chang (2017, July). Search-based neural structured learning for\n",
      "sequential question answering. In R. Barzilay and M.-Y. Kan (Eds.), Proceedings of the 55th Annual\n",
      "Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Vancouver,\n",
      "Canada, pp. 1821–1831. Association for Computational Linguistics.\n",
      "\n",
      "12\n",
      "\n",
      "\fJaegle, A., F. Gimeno, A. Brock, O. Vinyals, A. Zisserman, and J. Carreira (2021, 18–24 Jul). Per-\n",
      "ceiver: General perception with iterative attention. In M. Meila and T. Zhang (Eds.), Proceedings\n",
      "of the 38th International Conference on Machine Learning, Volume 139 of Proceedings of Machine\n",
      "Learning Research, pp. 4651–4664. PMLR.\n",
      "\n",
      "Jain, N., P. yeh Chiang, Y. Wen, J. Kirchenbauer, H.-M. Chu, G. Somepalli, B. R. Bartoldson,\n",
      "B. Kailkhura, A. Schwarzschild, A. Saha, M. Goldblum, J. Geiping, and T. Goldstein (2024).\n",
      "In The Twelfth International\n",
      "NEFTune: Noisy embeddings improve instruction finetuning.\n",
      "Conference on Learning Representations.\n",
      "\n",
      "Jhamtani, H. et al. (2018, October-November). Learning to describe differences between pairs of\n",
      "similar images. In E. Riloff, D. Chiang, J. Hockenmaier, and J. Tsujii (Eds.), Proceedings of the\n",
      "2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, pp.\n",
      "4024–4034. Association for Computational Linguistics.\n",
      "\n",
      "Jiang, A. Q., A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. de las Casas, F. Bressand,\n",
      "G. Lengyel, G. Lample, L. Saulnier, L. R. Lavaud, M.-A. Lachaux, P. Stock, T. L. Scao, T. Lavril,\n",
      "T. Wang, T. Lacroix, and W. E. Sayed (2023). Mistral 7b.\n",
      "\n",
      "Johnson, J., B. Hariharan, L. van der Maaten, L. Fei-Fei, C. L. Zitnick, and R. Girshick (2017). Clevr:\n",
      "A diagnostic dataset for compositional language and elementary visual reasoning. In 2017 IEEE\n",
      "Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1988–1997.\n",
      "\n",
      "Kafle, K., S. Cohen, B. Price, and C. Kanan (2018). Dvqa: Understanding data visualizations via\n",
      "\n",
      "question answering. In CVPR.\n",
      "\n",
      "Kahou, S. E., V. Michalski, A. Atkinson, A. Kadar, A. Trischler, and Y. Bengio (2018). Figureqa: An\n",
      "\n",
      "annotated figure dataset for visual reasoning.\n",
      "\n",
      "Karamcheti, S., S. Nair, A. Balakrishna, P. Liang, T. Kollar, and D. Sadigh (2024). Prismatic vlms:\n",
      "\n",
      "Investigating the design space of visually-conditioned language models.\n",
      "\n",
      "Kazemi, M., H. Alvari, A. Anand, J. Wu, X. Chen, and R. Soricut (2024). Geomverse: A systematic\n",
      "In Synthetic Data for Computer Vision\n",
      "\n",
      "evaluation of large models for geometric reasoning.\n",
      "Workshop @ CVPR 2024.\n",
      "\n",
      "Kembhavi, A., M. Salvato, E. Kolve, M. Seo, H. Hajishirzi, and A. Farhadi (2016). A diagram is\n",
      "worth a dozen images. In B. Leibe, J. Matas, N. Sebe, and M. Welling (Eds.), Computer Vision –\n",
      "ECCV 2016, Cham, pp. 235–251. Springer International Publishing.\n",
      "\n",
      "Kembhavi, A., M. Seo, D. Schwenk, J. Choi, A. Farhadi, and H. Hajishirzi (2017). Are you smarter\n",
      "than a sixth grader? textbook question answering for multimodal machine comprehension. In 2017\n",
      "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 5376–5384.\n",
      "\n",
      "Kiela, D., H. Firooz, A. Mohan, V. Goswami, A. Singh, P. Ringshia, and D. Testuggine (2020).\n",
      "The hateful memes challenge: Detecting hate speech in multimodal memes. In H. Larochelle,\n",
      "M. Ranzato, R. Hadsell, M. Balcan, and H. Lin (Eds.), Advances in Neural Information Processing\n",
      "Systems, Volume 33, pp. 2611–2624. Curran Associates, Inc.\n",
      "\n",
      "Kingma, D. and J. Ba (2015). Adam: A method for stochastic optimization.\n",
      "\n",
      "In International\n",
      "\n",
      "Conference on Learning Representations (ICLR), San Diega, CA, USA.\n",
      "\n",
      "Koh, J. Y., R. Salakhutdinov, and D. Fried (2023). Grounding language models to images for\n",
      "\n",
      "multimodal inputs and outputs.\n",
      "\n",
      "Lau, J., S. Gayen, A. Ben Abacha, and D. Demner-Fushman (2018, 11). A dataset of clinically\n",
      "\n",
      "generated visual questions and answers about radiology images. Scientific Data 5, 180251.\n",
      "\n",
      "Laurençon, H., L. Saulnier, T. Wang, C. Akiki, A. Villanova del Moral, T. Le Scao, L. Von Werra,\n",
      "C. Mou, E. González Ponferrada, H. Nguyen, J. Frohberg, M. Šaško, Q. Lhoest, A. McMillan-\n",
      "Major, G. Dupont, S. Biderman, A. Rogers, L. Ben allal, F. De Toni, G. Pistilli, O. Nguyen,\n",
      "S. Nikpoor, M. Masoud, P. Colombo, J. de la Rosa, P. Villegas, T. Thrush, S. Longpre, S. Nagel,\n",
      "L. Weber, M. Muñoz, J. Zhu, D. Van Strien, Z. Alyafeai, K. Almubarak, M. C. Vu, I. Gonzalez-\n",
      "Dios, A. Soroa, K. Lo, M. Dey, P. Ortiz Suarez, A. Gokaslan, S. Bose, D. Adelani, L. Phan, H. Tran,\n",
      "\n",
      "13\n",
      "\n",
      "\fI. Yu, S. Pai, J. Chim, V. Lepercq, S. Ilic, M. Mitchell, S. A. Luccioni, and Y. Jernite (2022).\n",
      "The bigscience roots corpus: A 1.6tb composite multilingual dataset. In S. Koyejo, S. Mohamed,\n",
      "A. Agarwal, D. Belgrave, K. Cho, and A. Oh (Eds.), Advances in Neural Information Processing\n",
      "Systems, Volume 35, pp. 31809–31826. Curran Associates, Inc.\n",
      "\n",
      "Laurençon, H., L. Saulnier, L. Tronchon, S. Bekman, A. Singh, A. Lozhkov, T. Wang, S. Karamcheti,\n",
      "A. M. Rush, D. Kiela, M. Cord, and V. Sanh (2023). OBELICS: An open web-scale filtered\n",
      "dataset of interleaved image-text documents. In Thirty-seventh Conference on Neural Information\n",
      "Processing Systems Datasets and Benchmarks Track.\n",
      "\n",
      "Laurençon, H., L. Tronchon, and V. Sanh (2024). Unlocking the conversion of web screenshots into\n",
      "\n",
      "html code with the websight dataset.\n",
      "\n",
      "Lee, B.-K., B. Park, C. W. Kim, and Y. M. Ro (2024). Moai: Mixture of all intelligence for large\n",
      "\n",
      "language and vision models.\n",
      "\n",
      "Lee, K., M. Joshi, I. Turc, H. Hu, F. Liu, J. Eisenschlos, U. Khandelwal, P. Shaw, M.-W. Chang,\n",
      "and K. Toutanova (2023). Pix2struct: screenshot parsing as pretraining for visual language under-\n",
      "standing. In Proceedings of the 40th International Conference on Machine Learning, ICML’23.\n",
      "JMLR.org.\n",
      "\n",
      "Li, B., R. Wang, G. Wang, Y. Ge, Y. Ge, and Y. Shan (2023). Seed-bench: Benchmarking multimodal\n",
      "\n",
      "llms with generative comprehension.\n",
      "\n",
      "Li, B., Y. Zhang, L. Chen, J. Wang, F. Pu, J. Yang, C. Li, and Z. Liu (2023). Mimic-it: Multi-modal\n",
      "\n",
      "in-context instruction tuning.\n",
      "\n",
      "Li, G., H. A. A. K. Hammoud, H. Itani, D. Khizbullin, and B. Ghanem (2023). CAMEL: Communica-\n",
      "tive agents for ”mind” exploration of large language model society. In Thirty-seventh Conference\n",
      "on Neural Information Processing Systems.\n",
      "\n",
      "Li, J., D. Li, S. Savarese, and S. Hoi (2023). Blip-2: bootstrapping language-image pre-training\n",
      "with frozen image encoders and large language models. In Proceedings of the 40th International\n",
      "Conference on Machine Learning, ICML’23. JMLR.org.\n",
      "\n",
      "Li, J., D. Li, C. Xiong, and S. Hoi (2022). Blip: Bootstrapping language-image pre-training for\n",
      "\n",
      "unified vision-language understanding and generation. In ICML.\n",
      "\n",
      "Li, L., Y. Yin, S. Li, L. Chen, P. Wang, S. Ren, M. Li, Y. Yang, J. Xu, X. Sun, L. Kong, and Q. Liu\n",
      "\n",
      "(2023). M3it: A large-scale dataset towards multi-modal multilingual instruction tuning.\n",
      "\n",
      "Li, Y., Y. Du, K. Zhou, J. Wang, X. Zhao, and J.-R. Wen (2023, December). Evaluating object\n",
      "hallucination in large vision-language models.\n",
      "In H. Bouamor, J. Pino, and K. Bali (Eds.),\n",
      "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing,\n",
      "Singapore, pp. 292–305. Association for Computational Linguistics.\n",
      "\n",
      "Li, Y., Y. Zhang, C. Wang, Z. Zhong, Y. Chen, R. Chu, S. Liu, and J. Jia (2024). Mini-gemini: Mining\n",
      "\n",
      "the potential of multi-modality vision language models.\n",
      "\n",
      "Li, Z., B. Yang, Q. Liu, Z. Ma, S. Zhang, J. Yang, Y. Sun, Y. Liu, and X. Bai (2024). Monkey: Image\n",
      "\n",
      "resolution and text label are important things for large multi-modal models.\n",
      "\n",
      "Lin, B., Z. Tang, Y. Ye, J. Cui, B. Zhu, P. Jin, J. Huang, J. Zhang, M. Ning, and L. Yuan (2024).\n",
      "\n",
      "Moe-llava: Mixture of experts for large vision-language models.\n",
      "\n",
      "Lin, J., H. Yin, W. Ping, Y. Lu, P. Molchanov, A. Tao, H. Mao, J. Kautz, M. Shoeybi, and S. Han\n",
      "\n",
      "(2024). Vila: On pre-training for visual language models.\n",
      "\n",
      "Lin, T.-Y., M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick (2014).\n",
      "Microsoft coco: Common objects in context. In D. Fleet, T. Pajdla, B. Schiele, and T. Tuytelaars\n",
      "(Eds.), Computer Vision – ECCV 2014, Cham, pp. 740–755. Springer International Publishing.\n",
      "\n",
      "Lin, Z., C. Liu, R. Zhang, P. Gao, L. Qiu, H. Xiao, H. Qiu, C. Lin, W. Shao, K. Chen, J. Han,\n",
      "S. Huang, Y. Zhang, X. He, H. Li, and Y. Qiao (2023). Sphinx: The joint mixing of weights, tasks,\n",
      "and visual embeddings for multi-modal large language models.\n",
      "\n",
      "14\n",
      "\n",
      "\fLindström, A. D. (2022). Clevr-math: A dataset for compositional language, visual, and mathematical\n",
      "\n",
      "reasoning.\n",
      "\n",
      "Liu, F., G. Emerson, and N. Collier (2023). Visual spatial reasoning. Transactions of the Association\n",
      "\n",
      "for Computational Linguistics 11, 635–651.\n",
      "\n",
      "Liu, H., C. Li, Y. Li, and Y. J. Lee (2023). Improved baselines with visual instruction tuning. In\n",
      "\n",
      "NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following.\n",
      "\n",
      "Liu, H., C. Li, Y. Li, B. Li, Y. Zhang, S. Shen, and Y. J. Lee (2024, January). Llava-next: Improved\n",
      "\n",
      "reasoning, ocr, and world knowledge.\n",
      "\n",
      "Liu, H., C. Li, Q. Wu, and Y. J. Lee (2023). Visual instruction tuning. In Thirty-seventh Conference\n",
      "\n",
      "on Neural Information Processing Systems.\n",
      "\n",
      "Liu, H., Q. You, X. Han, Y. Wang, B. Zhai, Y. Liu, Y. Tao, H. Huang, R. He, and H. Yang (2024).\n",
      "\n",
      "Infimm-hd: A leap forward in high-resolution multimodal understanding.\n",
      "\n",
      "Liu, S.-Y., C.-Y. Wang, H. Yin, P. Molchanov, Y.-C. F. Wang, K.-T. Cheng, and M.-H. Chen (2024).\n",
      "\n",
      "Dora: Weight-decomposed low-rank adaptation.\n",
      "\n",
      "Liu, T. and B. K. H. Low (2023). Goat: Fine-tuned llama outperforms gpt-4 on arithmetic tasks.\n",
      "\n",
      "Liu, Y., H. Duan, Y. Zhang, B. Li, S. Zhang, W. Zhao, Y. Yuan, J. Wang, C. He, Z. Liu, K. Chen, and\n",
      "\n",
      "D. Lin (2023). Mmbench: Is your multi-modal model an all-around player?\n",
      "\n",
      "Lu, H., W. Liu, B. Zhang, B. Wang, K. Dong, B. Liu, J. Sun, T. Ren, Z. Li, H. Yang, Y. Sun,\n",
      "C. Deng, H. Xu, Z. Xie, and C. Ruan (2024). Deepseek-vl: Towards real-world vision-language\n",
      "understanding.\n",
      "\n",
      "Lu, J., C. Clark, S. Lee, Z. Zhang, S. Khosla, R. Marten, D. Hoiem, and A. Kembhavi (2023).\n",
      "Unified-io 2: Scaling autoregressive multimodal models with vision, language, audio, and action.\n",
      "\n",
      "Lu, P., H. Bansal, T. Xia, J. Liu, C. Li, H. Hajishirzi, H. Cheng, K.-W. Chang, M. Galley, and J. Gao\n",
      "(2024). Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In\n",
      "International Conference on Learning Representations (ICLR).\n",
      "\n",
      "Lu, P., R. Gong, S. Jiang, L. Qiu, S. Huang, X. Liang, and S.-C. Zhu (2021). Inter-gps: Interpretable\n",
      "geometry problem solving with formal language and symbolic reasoning. In The Joint Confer-\n",
      "ence of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th\n",
      "International Joint Conference on Natural Language Processing (ACL-IJCNLP 2021).\n",
      "\n",
      "Lu, P., S. Mishra, T. Xia, L. Qiu, K.-W. Chang, S.-C. Zhu, O. Tafjord, P. Clark, and A. Kalyan (2022).\n",
      "Learn to explain: Multimodal reasoning via thought chains for science question answering. In\n",
      "S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (Eds.), Advances in Neural\n",
      "Information Processing Systems, Volume 35, pp. 2507–2521. Curran Associates, Inc.\n",
      "\n",
      "Lu, P., L. Qiu, K.-W. Chang, Y. N. Wu, S.-C. Zhu, T. Rajpurohit, P. Clark, and A. Kalyan (2023).\n",
      "Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning. In\n",
      "International Conference on Learning Representations (ICLR).\n",
      "\n",
      "Lu, P., L. Qiu, J. Chen, T. Xia, Y. Zhao, W. Zhang, Z. Yu, X. Liang, and S.-C. Zhu (2021). Iconqa:\n",
      "A new benchmark for abstract diagram understanding and visual language reasoning. In The\n",
      "35th Conference on Neural Information Processing Systems (NeurIPS) Track on Datasets and\n",
      "Benchmarks.\n",
      "\n",
      "Mañas, O., P. Rodriguez Lopez, S. Ahmadi, A. Nematzadeh, Y. Goyal, and A. Agrawal (2023,\n",
      "May). MAPL: Parameter-efficient adaptation of unimodal pre-trained models for vision-language\n",
      "few-shot prompting. In A. Vlachos and I. Augenstein (Eds.), Proceedings of the 17th Conference\n",
      "of the European Chapter of the Association for Computational Linguistics, Dubrovnik, Croatia, pp.\n",
      "2523–2548. Association for Computational Linguistics.\n",
      "\n",
      "Marino, K., M. Rastegari, A. Farhadi, and R. Mottaghi (2019). Ok-vqa: A visual question answer-\n",
      "ing benchmark requiring external knowledge. In Conference on Computer Vision and Pattern\n",
      "Recognition (CVPR).\n",
      "\n",
      "15\n",
      "\n",
      "\fMarti, U.-V. and H. Bunke (2002, 11). The iam-database: An english sentence database for offline\n",
      "handwriting recognition. International Journal on Document Analysis and Recognition 5, 39–46.\n",
      "\n",
      "Masry, A., D. Long, J. Q. Tan, S. Joty, and E. Hoque (2022, May). ChartQA: A benchmark for\n",
      "question answering about charts with visual and logical reasoning. In Findings of the Association\n",
      "for Computational Linguistics: ACL 2022, Dublin, Ireland, pp. 2263–2279. Association for\n",
      "Computational Linguistics.\n",
      "\n",
      "Mathew, M., V. Bagal, R. Tito, D. Karatzas, E. Valveny, and C. V. Jawahar (2022). Infographicvqa. In\n",
      "2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pp. 2582–2591.\n",
      "\n",
      "Mathew, M., D. Karatzas, and C. V. Jawahar (2021). Docvqa: A dataset for vqa on document images.\n",
      "In 2021 IEEE Winter Conference on Applications of Computer Vision (WACV), pp. 2199–2208.\n",
      "\n",
      "McKinzie, B., Z. Gan, J.-P. Fauconnier, S. Dodge, B. Zhang, P. Dufter, D. Shah, X. Du, F. Peng,\n",
      "F. Weers, A. Belyi, H. Zhang, K. Singh, D. Kang, A. Jain, H. Hè, M. Schwarzer, T. Gunter,\n",
      "X. Kong, A. Zhang, J. Wang, C. Wang, N. Du, T. Lei, S. Wiseman, G. Yin, M. Lee, Z. Wang,\n",
      "R. Pang, P. Grasch, A. Toshev, and Y. Yang (2024). Mm1: Methods, analysis & insights from\n",
      "multimodal llm pre-training.\n",
      "\n",
      "Methani, N., P. Ganguly, M. M. Khapra, and P. Kumar (2020, March). Plotqa: Reasoning over\n",
      "scientific plots. In The IEEE Winter Conference on Applications of Computer Vision (WACV).\n",
      "\n",
      "Mishra, A., S. Shekhar, A. K. Singh, and A. Chakraborty (2019). Ocr-vqa: Visual question answering\n",
      "by reading text in images. In 2019 International Conference on Document Analysis and Recognition\n",
      "(ICDAR), pp. 947–952.\n",
      "\n",
      "Mitra, A., H. Khanpour, C. Rosset, and A. Awadallah (2024). Orca-math: Unlocking the potential of\n",
      "\n",
      "slms in grade school math.\n",
      "\n",
      "Obeid, J. and E. Hoque (2020, December). Chart-to-text: Generating natural language descriptions\n",
      "for charts by adapting the transformer model. In B. Davis, Y. Graham, J. Kelleher, and Y. Sripada\n",
      "(Eds.), Proceedings of the 13th International Conference on Natural Language Generation, Dublin,\n",
      "Ireland, pp. 138–147. Association for Computational Linguistics.\n",
      "\n",
      "OpenAI (2024). Gpt-4 technical report.\n",
      "\n",
      "Pasupat, P. and P. Liang (2015, July). Compositional semantic parsing on semi-structured tables. In\n",
      "C. Zong and M. Strube (Eds.), Proceedings of the 53rd Annual Meeting of the Association for\n",
      "Computational Linguistics and the 7th International Joint Conference on Natural Language Pro-\n",
      "cessing (Volume 1: Long Papers), Beijing, China, pp. 1470–1480. Association for Computational\n",
      "Linguistics.\n",
      "\n",
      "Penedo, G., Q. Malartic, D. Hesslow, R. Cojocaru, H. Alobeidli, A. Cappelli, B. Pannier, E. Al-\n",
      "mazrouei, and J. Launay (2023). The refinedweb dataset for falcon LLM: Outperforming curated\n",
      "corpora with web data only. In Thirty-seventh Conference on Neural Information Processing\n",
      "Systems Datasets and Benchmarks Track.\n",
      "\n",
      "Pont-Tuset, J., J. Uijlings, S. Changpinyo, R. Soricut, and V. Ferrari (2020). Connecting vision and\n",
      "language with localized narratives. In A. Vedaldi, H. Bischof, T. Brox, and J.-M. Frahm (Eds.),\n",
      "Computer Vision – ECCV 2020, Cham, pp. 647–664. Springer International Publishing.\n",
      "\n",
      "Radford, A., J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin,\n",
      "J. Clark, G. Krueger, and I. Sutskever (2021). Learning transferable visual models from natural\n",
      "language supervision. In International Conference on Machine Learning.\n",
      "\n",
      "Ren, M., R. Kiros, and R. Zemel (2015). Exploring models and data for image question answering.\n",
      "In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett (Eds.), Advances in Neural\n",
      "Information Processing Systems, Volume 28. Curran Associates, Inc.\n",
      "\n",
      "Sanh, V., A. Webson, C. Raffel, S. Bach, L. Sutawika, Z. Alyafeai, A. Chaffin, A. Stiegler, A. Raja,\n",
      "M. Dey, M. S. Bari, C. Xu, U. Thakker, S. S. Sharma, E. Szczechla, T. Kim, G. Chhablani,\n",
      "N. Nayak, D. Datta, J. Chang, M. T.-J. Jiang, H. Wang, M. Manica, S. Shen, Z. X. Yong, H. Pandey,\n",
      "\n",
      "16\n",
      "\n",
      "\fR. Bawden, T. Wang, T. Neeraj, J. Rozen, A. Sharma, A. Santilli, T. Fevry, J. A. Fries, R. Teehan,\n",
      "T. L. Scao, S. Biderman, L. Gao, T. Wolf, and A. M. Rush (2022). Multitask prompted training\n",
      "enables zero-shot task generalization. In International Conference on Learning Representations.\n",
      "\n",
      "Schuhmann, C., R. Beaumont, R. Vencu, C. Gordon, R. Wightman, M. Cherti, T. Coombes, A. Katta,\n",
      "C. Mullis, M. Wortsman, P. Schramowski, S. Kundurthy, K. Crowson, L. Schmidt, R. Kaczmarczyk,\n",
      "and J. Jitsev (2022). Laion-5b: An open large-scale dataset for training next generation image-text\n",
      "models. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (Eds.), Advances\n",
      "in Neural Information Processing Systems, Volume 35, pp. 25278–25294. Curran Associates, Inc.\n",
      "\n",
      "Schuhmann, C., R. Vencu, R. Beaumont, R. Kaczmarczyk, C. Mullis, A. Katta, T. Coombes, J. Jitsev,\n",
      "and A. Komatsuzaki (2021). Laion-400m: Open dataset of clip-filtered 400 million image-text\n",
      "pairs.\n",
      "\n",
      "Schwenk, D., A. Khandelwal, C. Clark, K. Marino, and R. Mottaghi (2022). A-okvqa: A benchmark\n",
      "for visual question answering using world knowledge. In Computer Vision – ECCV 2022: 17th\n",
      "European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part VIII, Berlin,\n",
      "Heidelberg, pp. 146–162. Springer-Verlag.\n",
      "\n",
      "Sharma, P., N. Ding, S. Goodman, and R. Soricut (2018). Conceptual captions: A cleaned, hyper-\n",
      "\n",
      "nymed, image alt-text dataset for automatic image captioning. In Proceedings of ACL.\n",
      "\n",
      "Shayegani, E., Y. Dong, and N. Abu-Ghazaleh (2024). Jailbreak in pieces: Compositional adversarial\n",
      "attacks on multi-modal language models. In The Twelfth International Conference on Learning\n",
      "Representations.\n",
      "\n",
      "Shukor, M., C. Dancette, and M. Cord (2023, oct). ep-alm: Efficient perceptual augmentation of\n",
      "language models. In 2023 IEEE/CVF International Conference on Computer Vision (ICCV), Los\n",
      "Alamitos, CA, USA, pp. 21999–22012. IEEE Computer Society.\n",
      "\n",
      "Sidorov, O., R. Hu, M. Rohrbach, and A. Singh (2020). Textcaps: A dataset for image captioning with\n",
      "reading comprehension. In A. Vedaldi, H. Bischof, T. Brox, and J.-M. Frahm (Eds.), Computer\n",
      "Vision – ECCV 2020, Cham, pp. 742–758. Springer International Publishing.\n",
      "\n",
      "Singh, A., R. Hu, V. Goswami, G. Couairon, W. Galuba, M. Rohrbach, and D. Kiela (2022). Flava: A\n",
      "foundational language and vision alignment model. In 2022 IEEE/CVF Conference on Computer\n",
      "Vision and Pattern Recognition (CVPR), pp. 15617–15629.\n",
      "\n",
      "Singh, A., V. Natarjan, M. Shah, Y. Jiang, X. Chen, D. Parikh, and M. Rohrbach (2019). Towards\n",
      "vqa models that can read. In Proceedings of the IEEE Conference on Computer Vision and Pattern\n",
      "Recognition, pp. 8317–8326.\n",
      "\n",
      "Srinivasan, K., K. Raman, J. Chen, M. Bendersky, and M. Najork (2021). Wit: Wikipedia-based\n",
      "image text dataset for multimodal multilingual machine learning. In Proceedings of the 44th\n",
      "International ACM SIGIR Conference on Research and Development in Information Retrieval,\n",
      "SIGIR ’21, New York, NY, USA, pp. 2443–2449. Association for Computing Machinery.\n",
      "\n",
      "Suhr, A., S. Zhou, A. Zhang, I. Zhang, H. Bai, and Y. Artzi (2019, July). A corpus for reasoning\n",
      "about natural language grounded in photographs. In A. Korhonen, D. Traum, and L. Màrquez\n",
      "(Eds.), Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,\n",
      "Florence, Italy, pp. 6418–6428. Association for Computational Linguistics.\n",
      "\n",
      "Sun, Q., Y. Cui, X. Zhang, F. Zhang, Q. Yu, Z. Luo, Y. Wang, Y. Rao, J. Liu, T. Huang, and X. Wang\n",
      "\n",
      "(2023). Generative multimodal models are in-context learners.\n",
      "\n",
      "Sun, Q., Y. Fang, L. Wu, X. Wang, and Y. Cao (2023). Eva-clip: Improved training techniques for\n",
      "\n",
      "clip at scale.\n",
      "\n",
      "Sun, Z., S. Shen, S. Cao, H. Liu, C. Li, Y. Shen, C. Gan, L.-Y. Gui, Y.-X. Wang, Y. Yang, K. Keutzer,\n",
      "\n",
      "and T. Darrell (2023). Aligning large multimodal models with factually augmented rlhf.\n",
      "\n",
      "Tanaka, R., K. Nishida, and S. Yoshida (2021). Visualmrc: Machine reading comprehension on\n",
      "\n",
      "document images. In AAAI.\n",
      "\n",
      "17\n",
      "\n",
      "\fTang, B. J., A. Boggust, and A. Satyanarayan (2023). VisText: A Benchmark for Semantically Rich\n",
      "Chart Captioning. In The Annual Meeting of the Association for Computational Linguistics (ACL).\n",
      "\n",
      "Teknium (2023). Openhermes 2.5: An open dataset of synthetic data for generalist llm assistants.\n",
      "\n",
      "Thiel, D. (2023). Identifying and eliminating csam in generative ml training data and models.\n",
      "\n",
      "Touvron, H., T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal,\n",
      "E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample (2023). Llama: Open and\n",
      "efficient foundation language models.\n",
      "\n",
      "Touvron, H., L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra,\n",
      "P. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. C. Ferrer, M. Chen, G. Cucurull, D. Esiobu,\n",
      "J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini,\n",
      "R. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M.-A.\n",
      "Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra,\n",
      "I. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M.\n",
      "Smith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan,\n",
      "I. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and\n",
      "T. Scialom (2023). Llama 2: Open foundation and fine-tuned chat models.\n",
      "\n",
      "Vallaeys, T., M. Shukor, M. Cord, and J. Verbeek (2024). Improved baselines for data-efficient\n",
      "\n",
      "perceptual augmentation of llms.\n",
      "\n",
      "Wang, B., G. Li, X. Zhou, Z. Chen, T. Grossman, and Y. Li (2021). Screen2words: Automatic\n",
      "mobile ui summarization with multimodal learning. In The 34th Annual ACM Symposium on User\n",
      "Interface Software and Technology, UIST ’21, New York, NY, USA, pp. 498–510. Association for\n",
      "Computing Machinery.\n",
      "\n",
      "Wang, W., Q. Lv, W. Yu, W. Hong, J. Qi, Y. Wang, J. Ji, Z. Yang, L. Zhao, X. Song, J. Xu, B. Xu, J. Li,\n",
      "Y. Dong, M. Ding, and J. Tang (2024). Cogvlm: Visual expert for pretrained language models.\n",
      "\n",
      "Wei, J., M. Bosma, V. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V. Le (2022).\n",
      "In International Conference on Learning\n",
      "\n",
      "Finetuned language models are zero-shot learners.\n",
      "Representations.\n",
      "\n",
      "Xiao, J., Z. Xu, A. Yuille, S. Yan, and B. Wang (2024). Palm2-vadapter: Progressively aligned\n",
      "\n",
      "language model makes a strong vision-language adapter.\n",
      "\n",
      "Young, P., A. Lai, M. Hodosh, and J. Hockenmaier (2014). From image descriptions to visual\n",
      "denotations: New similarity metrics for semantic inference over event descriptions. Transactions\n",
      "of the Association for Computational Linguistics 2, 67–78.\n",
      "\n",
      "Yu, L., W. Jiang, H. Shi, J. YU, Z. Liu, Y. Zhang, J. Kwok, Z. Li, A. Weller, and W. Liu (2024).\n",
      "Metamath: Bootstrap your own mathematical questions for large language models. In The Twelfth\n",
      "International Conference on Learning Representations.\n",
      "\n",
      "Yue, X., Y. Ni, K. Zhang, T. Zheng, R. Liu, G. Zhang, S. Stevens, D. Jiang, W. Ren, Y. Sun, C. Wei,\n",
      "B. Yu, R. Yuan, R. Sun, M. Yin, B. Zheng, Z. Yang, Y. Liu, W. Huang, H. Sun, Y. Su, and W. Chen\n",
      "(2024). Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark\n",
      "for expert agi. In Proceedings of CVPR.\n",
      "\n",
      "Yue, X., X. Qu, G. Zhang, Y. Fu, W. Huang, H. Sun, Y. Su, and W. Chen (2024). MAmmoTH:\n",
      "Building math generalist models through hybrid instruction tuning. In The Twelfth International\n",
      "Conference on Learning Representations.\n",
      "\n",
      "Zhai, X., B. Mustafa, A. Kolesnikov, and L. Beyer (2023). Sigmoid loss for language image\n",
      "\n",
      "pre-training.\n",
      "\n",
      "Zhang, C., F. Gao, B. Jia, Y. Zhu, and S.-C. Zhu (2019). Raven: A dataset for relational and\n",
      "analogical visual reasoning. In Proceedings of the IEEE Conference on Computer Vision and\n",
      "Pattern Recognition (CVPR).\n",
      "\n",
      "18\n",
      "\n",
      "\fZhang, X., C. Wu, Z. Zhao, W. Lin, Y. Zhang, Y. Wang, and W. Xie (2023). Pmc-vqa: Visual\n",
      "\n",
      "instruction tuning for medical visual question answering.\n",
      "\n",
      "Zhao, Y., Y. Li, C. Li, and R. Zhang (2022, May). MultiHiertt: Numerical reasoning over multi hier-\n",
      "archical tabular and textual data. In Proceedings of the 60th Annual Meeting of the Association for\n",
      "Computational Linguistics (Volume 1: Long Papers), Dublin, Ireland, pp. 6588–6600. Association\n",
      "for Computational Linguistics.\n",
      "\n",
      "Zhao, Y., C. Zhao, L. Nan, Z. Qi, W. Zhang, X. Tang, B. Mi, and D. Radev (2023, July). RobuT: A\n",
      "systematic study of table QA robustness against human-annotated adversarial perturbations. In\n",
      "A. Rogers, J. Boyd-Graber, and N. Okazaki (Eds.), Proceedings of the 61st Annual Meeting of\n",
      "the Association for Computational Linguistics (Volume 1: Long Papers), Toronto, Canada, pp.\n",
      "6064–6081. Association for Computational Linguistics.\n",
      "\n",
      "Zhong, V., C. Xiong, and R. Socher (2017). Seq2sql: Generating structured queries from natural\n",
      "\n",
      "language using reinforcement learning.\n",
      "\n",
      "Zhou, B., Y. Hu, X. Weng, J. Jia, J. Luo, X. Liu, J. Wu, and L. Huang (2024). Tinyllava: A framework\n",
      "\n",
      "of small-scale large multimodal models.\n",
      "\n",
      "Zhou, C., P. Liu, P. Xu, S. Iyer, J. Sun, Y. Mao, X. Ma, A. Efrat, P. Yu, L. YU, S. Zhang, G. Ghosh,\n",
      "M. Lewis, L. Zettlemoyer, and O. Levy (2023). LIMA: Less is more for alignment. In Thirty-\n",
      "seventh Conference on Neural Information Processing Systems.\n",
      "\n",
      "Zhu, F., W. Lei, Y. Huang, C. Wang, S. Zhang, J. Lv, F. Feng, and T.-S. Chua (2021, August).\n",
      "TAT-QA: A question answering benchmark on a hybrid of tabular and textual content in finance. In\n",
      "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the\n",
      "11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),\n",
      "Online, pp. 3277–3287. Association for Computational Linguistics.\n",
      "\n",
      "Zhu, W., J. Hessel, A. Awadalla, S. Y. Gadre, J. Dodge, A. Fang, Y. Yu, L. Schmidt, W. Y. Wang, and\n",
      "Y. Choi (2023). Multimodal c4: An open, billion-scale corpus of images interleaved with text. In\n",
      "Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks\n",
      "Track.\n",
      "\n",
      "Zhu, Y., O. Groth, M. Bernstein, and L. Fei-Fei (2016). Visual7W: Grounded Question Answering in\n",
      "\n",
      "Images. In IEEE Conference on Computer Vision and Pattern Recognition.\n",
      "\n",
      "19\n",
      "\n",
      "\fA Appendix\n",
      "\n",
      "A.1 Further experimental details of the ablations\n",
      "\n",
      "A.1.1 Cross-attention vs. fully autoregressive architectures\n",
      "\n",
      "We apply LoRA modules to the LLM for the fully autoregressive architecture and to the cross-\n",
      "attention modules and the LLM for the cross-attention architecture. In Figure 4, we report the average\n",
      "performance with respect to the number of steps, the number of images, as well as the number of text\n",
      "tokens. We see an improvement across the board with the fully autoregressive architecture. Comparing\n",
      "the average score with these different axes is essential because the cross-attention architecture feeds a\n",
      "single token per image to the language model, against 64 for the fully autoregressive architecture with\n",
      "perceiver pooling. This implies that for the same training sequence length, the number of images and\n",
      "text tokens is different for the two architectures. Equivalently, the same multimodal document will\n",
      "yield different sequence lengths. Even though we fix the batch size in the comparison, the number of\n",
      "text tokens and number of images grow at different paces under the two architectures.\n",
      "\n",
      "Figure 4: Comparison of the cross-attention and fully autoregressive architectures through the number\n",
      "of steps, the number of images and the number of text tokens.\n",
      "\n",
      "A.1.2 Comparing various vision backbones\n",
      "\n",
      "We present in Table 10 the detailed results of comparing multiple vision backbones. While EVA-CLIP-\n",
      "5B performs similarly to SigLIP-SO400M, we emphasize that it has 11 times more parameters. We\n",
      "also noticed in early experiments that TextVQA is the most sensitive benchmark to image resolution,\n",
      "which accounts for the performance increase.\n",
      "\n",
      "VE backbone\n",
      "\n",
      "Size\n",
      "\n",
      "Res. Avg. score VQAv2 OKVQA TextVQA COCO\n",
      "\n",
      "600M 224\n",
      "CLIP-ViT-H\n",
      "EVA-CLIP-5B\n",
      "224\n",
      "4.4B\n",
      "SigLIP-SO400M 400M 384\n",
      "\n",
      "57.4\n",
      "60.2\n",
      "60.6\n",
      "\n",
      "52.4\n",
      "53.4\n",
      "53.6\n",
      "\n",
      "41.7\n",
      "43.3\n",
      "43.4\n",
      "\n",
      "28.2\n",
      "30.4\n",
      "33.8\n",
      "\n",
      "107.5\n",
      "113.7\n",
      "111.6\n",
      "\n",
      "Table 10: Detailed results of ablation on the vision encoder backbone\n",
      "\n",
      "A.1.3 Comparing various pooling strategies\n",
      "\n",
      "We compare multiple pooling strategies: a simple linear layer that takes the flattened sequence of\n",
      "vision hidden states and projects it into a shorter sequence of visual tokens, as well as a Mapping\n",
      "Network (Mañas et al., 2023). The perceiver resampler significantly outperforms these two options\n",
      "(see Table 11).\n",
      "\n",
      "We also ablate the number of layers in the perceiver resampler, and find no statistically significant\n",
      "differences when increasing the number of layers, similarly to results from Xiao et al. (2024). We\n",
      "settle on 3 layers out of caution to avoid any potential capacity bottleneck.\n",
      "\n",
      "Finally, we add a 2-layer modality projection MLP on top of the vision encoder hidden states to\n",
      "project the vision hidden dimension to the language model hidden dimension prior to the perceiver\n",
      "resampler. These changes yield better performance as well (see Table 13).\n",
      "\n",
      "20\n",
      "\n",
      "\fVision-language Connector\n",
      "\n",
      "Avg. score\n",
      "\n",
      "Linear Projection\n",
      "Mapping Network (Mañas et al., 2023)\n",
      "Perceiver\n",
      "\n",
      "44.5\n",
      "51.8\n",
      "60.3\n",
      "\n",
      "Table 11: Ablation on the modality projection\n",
      "\n",
      "Num. perceiver layers Avg. score\n",
      "\n",
      "1\n",
      "3\n",
      "12\n",
      "\n",
      "69.3\n",
      "68.6\n",
      "69.0\n",
      "\n",
      "Table 12: Ablation on the number of perceiver resampler layers\n",
      "\n",
      "MLP modality projection Avg. score\n",
      "\n",
      "W/\n",
      "W/o\n",
      "\n",
      "71.4\n",
      "69.6\n",
      "\n",
      "Table 13: Ablation on the addition of a modality projection before the perceiver resampler\n",
      "\n",
      "A.1.4 Ablations on OCR data\n",
      "\n",
      "We hypothesize that adding PDF documents helps the model learn to read text from images. In Table 7,\n",
      "we compare checkpoints trained with and without OCR documents, along with image resolution\n",
      "increase to ensure that the text is legible. We do not observe statistically significant differences when\n",
      "evaluating checkpoints in zero or few shot. Instead, we fine-tune the checkpoints on DocVQA for\n",
      "500 steps with a learning rate of 1e − 5, leading to checkpoints showing much stronger differences.\n",
      "\n",
      "A.2 Details of the instruction fine-tuning\n",
      "\n",
      "A.2.1 Statistics of The Cauldron\n",
      "\n",
      "In Table 14, we present the statistics of the datasets included in The Cauldron, as well as the text-only\n",
      "instruction datasets used for the supervised fine-tuning. For each dataset, we give the number of\n",
      "different images it contains, the number of question-answer pairs, the total number of tokens for the\n",
      "answers in the question-answer pairs, and the selected percentage of tokens it represents in our final\n",
      "mixture after upsampling or downsampling.\n",
      "\n",
      "Dataset\n",
      "\n",
      "# images\n",
      "\n",
      "# Q/A pairs\n",
      "\n",
      "# tokens\n",
      "\n",
      "% mixture\n",
      "\n",
      "General visual question answering\n",
      "VQAv2 (Goyal et al., 2017)\n",
      "COCO-QA (Ren et al., 2015)\n",
      "Visual7W (Zhu et al., 2016)\n",
      "A-OKVQA (Schwenk et al., 2022)\n",
      "TallyQA (Acharya et al., 2019)\n",
      "OK-VQA (Marino et al., 2019)\n",
      "HatefulMemes (Kiela et al., 2020)\n",
      "VQA-RAD (Lau et al., 2018)\n",
      "\n",
      "Captioning\n",
      "LNarratives (Pont-Tuset et al., 2020)\n",
      "Screen2Words (Wang et al., 2021)\n",
      "VSR (Liu et al., 2023)\n",
      "\n",
      "82,772\n",
      "46,287\n",
      "14,366\n",
      "16,539\n",
      "98,680\n",
      "8,998\n",
      "8,500\n",
      "313\n",
      "\n",
      "443,757\n",
      "78,736\n",
      "69,817\n",
      "17,056\n",
      "183,986\n",
      "9,009\n",
      "8,500\n",
      "1,793\n",
      "\n",
      "1,595,929\n",
      "286,982\n",
      "279,268\n",
      "236,492\n",
      "738,254\n",
      "38,853\n",
      "25,500\n",
      "8,418\n",
      "\n",
      "507,444\n",
      "15,730\n",
      "2,157\n",
      "\n",
      "507,444\n",
      "15,743\n",
      "3,354\n",
      "\n",
      "21,328,731\n",
      "143,103\n",
      "10,062\n",
      "\n",
      "5.72%\n",
      "1.47%\n",
      "1.43%\n",
      "1.21%\n",
      "0.57%\n",
      "0.40%\n",
      "0.13%\n",
      "0.09%\n",
      "\n",
      "4.56%\n",
      "0.37%\n",
      "0.21%\n",
      "\n",
      "OCR, document understanding, text transcription\n",
      "RenderedText9\n",
      "999,000\n",
      "10,189\n",
      "DocVQA (Mathew et al., 2021)\n",
      "\n",
      "999,000\n",
      "39,463\n",
      "\n",
      "27,207,774\n",
      "337,829\n",
      "\n",
      "5.57%\n",
      "3.46%\n",
      "\n",
      "9https://huggingface.co/datasets/wendlerc/RenderedText\n",
      "\n",
      "21\n",
      "\n",
      "\fTextCaps (Sidorov et al., 2020)\n",
      "TextVQA (Singh et al., 2019)\n",
      "ST-VQA (Biten et al., 2019)\n",
      "OCR-VQA (Mishra et al., 2019)\n",
      "VisualMRC (Tanaka et al., 2021)\n",
      "IAM (Marti and Bunke, 2002)\n",
      "InfoVQA (Mathew et al., 2022)\n",
      "Diagram image-to-text10\n",
      "\n",
      "Chart/figure understanding\n",
      "Chart2Text (Obeid and Hoque, 2020)\n",
      "DVQA (Kafle et al., 2018)\n",
      "VisText (Tang et al., 2023)\n",
      "ChartQA (Masry et al., 2022)\n",
      "PlotQA (Methani et al., 2020)\n",
      "FigureQA (Kahou et al., 2018)\n",
      "MapQA (Chang et al., 2022)\n",
      "\n",
      "Table understanding\n",
      "TabMWP (Lu et al., 2023)\n",
      "TAT-QA (Zhu et al., 2021)\n",
      "HiTab (Cheng et al., 2022)\n",
      "MultiHiertt (Zhao et al., 2022)\n",
      "FinQA (Chen et al., 2021)\n",
      "WikiSQL (Zhong et al., 2017)\n",
      "SQA (Iyyer et al., 2017)\n",
      "WTQ (Pasupat and Liang, 2015)\n",
      "\n",
      "Reasoning, logic, maths\n",
      "GeomVerse (Kazemi et al., 2024)\n",
      "CLEVR-Math (Lindström, 2022)\n",
      "CLEVR (Johnson et al., 2017)\n",
      "IconQA (Lu et al., 2021)\n",
      "RAVEN (Zhang et al., 2019)\n",
      "Inter-GPs (Lu et al., 2021)\n",
      "\n",
      "Textbook/academic questions\n",
      "AI2D (Kembhavi et al., 2016)\n",
      "TQA (Kembhavi et al., 2017)\n",
      "ScienceQA (Lu et al., 2022)\n",
      "\n",
      "Differences between 2 images\n",
      "NLVR2 (Suhr et al., 2019)\n",
      "GSD (Li et al., 2023)\n",
      "Spot the diff (Jhamtani et al., 2018)\n",
      "\n",
      "Screenshot to code\n",
      "WebSight (Laurençon et al., 2024)\n",
      "DaTikz (Belouadi et al., 2024)\n",
      "\n",
      "21,953\n",
      "21,953\n",
      "17,247\n",
      "165,746\n",
      "3,027\n",
      "5,663\n",
      "2,118\n",
      "300\n",
      "\n",
      "26,985\n",
      "200,000\n",
      "7,057\n",
      "18,271\n",
      "157,070\n",
      "100,000\n",
      "37,417\n",
      "\n",
      "22,729\n",
      "2,199\n",
      "2,500\n",
      "7,619\n",
      "5,276\n",
      "74,989\n",
      "8,514\n",
      "38,246\n",
      "\n",
      "9,303\n",
      "70,000\n",
      "70,000\n",
      "27,315\n",
      "42,000\n",
      "1,451\n",
      "\n",
      "3,099\n",
      "1,496\n",
      "4,985\n",
      "\n",
      "50,426\n",
      "70,939\n",
      "8,566\n",
      "\n",
      "21,953\n",
      "34,602\n",
      "23,121\n",
      "801,579\n",
      "11,988\n",
      "5,663\n",
      "10,074\n",
      "300\n",
      "\n",
      "389,658\n",
      "181,918\n",
      "127,846\n",
      "6,073,824\n",
      "168,828\n",
      "144,216\n",
      "61,048\n",
      "22,196\n",
      "\n",
      "30,242\n",
      "2,325,316\n",
      "9,969\n",
      "28,299\n",
      "20,249,479\n",
      "1,327,368\n",
      "483,416\n",
      "\n",
      "2,852,827\n",
      "8,346,234\n",
      "1,245,485\n",
      "185,835\n",
      "8478299.278\n",
      "3,982,104\n",
      "6,470,485\n",
      "\n",
      "23,059\n",
      "13,215\n",
      "7,782\n",
      "7,830\n",
      "6,251\n",
      "86,202\n",
      "34,141\n",
      "44,096\n",
      "\n",
      "9,339\n",
      "788,650\n",
      "699,989\n",
      "29,859\n",
      "42,000\n",
      "2,101\n",
      "\n",
      "9,708\n",
      "6,501\n",
      "6,218\n",
      "\n",
      "1,948,166\n",
      "283,776\n",
      "351,299\n",
      "267,615\n",
      "242,561\n",
      "9,680,673\n",
      "1,894,824\n",
      "6,677,013\n",
      "\n",
      "2,489,459\n",
      "3,184,656\n",
      "2,396,781\n",
      "112,969\n",
      "105,081\n",
      "8,404\n",
      "\n",
      "38,832\n",
      "26,004\n",
      "24,872\n",
      "\n",
      "86,373\n",
      "141,869\n",
      "9,524\n",
      "\n",
      "259,119\n",
      "4,637,229\n",
      "221,477\n",
      "\n",
      "2.00%\n",
      "1.86%\n",
      "1.31%\n",
      "0.93%\n",
      "0.86%\n",
      "0.74%\n",
      "0.63%\n",
      "0.11%\n",
      "\n",
      "4.38%\n",
      "4.27%\n",
      "1.91%\n",
      "1.90%\n",
      "0.65%\n",
      "0.61%\n",
      "0.33%\n",
      "\n",
      "2.49%\n",
      "2.18%\n",
      "1.80%\n",
      "1.37%\n",
      "0.99%\n",
      "0.99%\n",
      "0.97%\n",
      "0.51%\n",
      "\n",
      "3.83%\n",
      "3.26%\n",
      "1.23%\n",
      "1.16%\n",
      "0.67%\n",
      "0.17%\n",
      "\n",
      "0.80%\n",
      "0.53%\n",
      "0.25%\n",
      "\n",
      "1.33%\n",
      "0.48%\n",
      "0.57%\n",
      "\n",
      "500,000\n",
      "47,974\n",
      "\n",
      "500,000\n",
      "48,296\n",
      "\n",
      "276,743,299\n",
      "59,556,252\n",
      "\n",
      "0.28%\n",
      "0.03%\n",
      "\n",
      "Text-only general instructions, math problems, arithmetic calculations\n",
      "OpenHermes-2.5 (Teknium, 2023)\n",
      "LIMA (Zhou et al., 2023)\n",
      "Dolly (Conover et al., 2023)\n",
      "MetaMathQA (Yu et al., 2024)\n",
      "MathInstruct (Yue et al., 2024)\n",
      "\n",
      "1,006,223\n",
      "1,052\n",
      "14,972\n",
      "395,000\n",
      "261,781\n",
      "\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "\n",
      "248,553,747\n",
      "633,867\n",
      "1,329,999\n",
      "74,328,255\n",
      "45,393,559\n",
      "\n",
      "12.73%\n",
      "0.81%\n",
      "0.68%\n",
      "3.81%\n",
      "2.33%\n",
      "\n",
      "10https://huggingface.co/datasets/Kamizuru00/diagram_image_to_text\n",
      "\n",
      "22\n",
      "\n",
      "\fOrcaMath (Mitra et al., 2024)\n",
      "CamelAIMath (Li et al., 2023)\n",
      "AtlasMathSets11\n",
      "Goat (Liu and Low, 2023)\n",
      "\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "\n",
      "200,031\n",
      "49,744\n",
      "17,807,579\n",
      "1,746,300\n",
      "\n",
      "63,780,702\n",
      "21,873,629\n",
      "455,411,624\n",
      "167,695,693\n",
      "\n",
      "1.63%\n",
      "0.06%\n",
      "3.50%\n",
      "0.86%\n",
      "\n",
      "Table 14: The statistics of datasets used for instruction fine-tuning. # tokens is the total number of\n",
      "tokens for each dataset for the answers only. % mixture is our selected percentage of answer tokens\n",
      "for each dataset in the final mixture.\n",
      "\n",
      "A.3 Details of the evaluations\n",
      "\n",
      "A.3.1 Evaluation setup\n",
      "\n",
      "We perform all evaluations with a batch size of 1 and greedy decoding.\n",
      "\n",
      "For the multi-choice questions in MMMU, MathVista, MMBench, we evaluate with the same prompt\n",
      "used for similar types of datasets during the instruction fine-tuning:\n",
      "\n",
      "Question: {question}\n",
      "Choices:\n",
      "A. {choice_a}\n",
      "B. {choice_b}\n",
      "C. {choice_c}\n",
      "...\n",
      "Answer with the letter.\n",
      "\n",
      "For the open-ended questions in TextVQA, DocVQA, and VQAv2, we evaluate with the prompt:\n",
      "\n",
      "Question: {question}\n",
      "Give a very brief answer.\n",
      "\n",
      "We use the stop words Question, User, <end_of_utterance> and <eos> to stop a generation.\n",
      "\n",
      "A.3.2 Expanded evaluation table\n",
      "\n",
      "We report the expanded evaluation of Idefics2 and the comparison to other models in Table 15.\n",
      "This includes scores on VQAv2 (Goyal et al., 2017), which is widely adopted for evaluation. We\n",
      "acknowledge, though, that the metric used for the open-ended visual question answering benchmarks\n",
      "strongly penalizes models that do not generate in the same format as the ground truth. For example,\n",
      "answering \"large\" when the ground truth is \"big\" or more verbose reformulations will be counted as\n",
      "incorrect. Our manual qualitative analysis reveals that on benchmarks like VQAv2, the generations\n",
      "of two models differing by 5 points would be barely noticeable. This problem is less concerning for\n",
      "other open-ended benchmarks like TextVQA or DocVQA which require finding a text in an image,\n",
      "making the expected answer less prone to ambiguity.\n",
      "\n",
      "A.3.3 Qualitative evaluation\n",
      "\n",
      "We show in Figures 5, 6, and 7, examples of generations with Idefics2-chatty.\n",
      "\n",
      "A.4 Red-teaming\n",
      "\n",
      "In the context of a red-teaming exercise, our objective is to evaluate the propensity of the model to\n",
      "generate inaccurate, biased, or offensive responses. We evaluate more specifically the chat-optimized\n",
      "checkpoint12.\n",
      "\n",
      "11https://huggingface.co/datasets/AtlasUnified/atlas-math-sets\n",
      "12https://huggingface.co/HuggingFaceM4/idefics2-8b-chatty\n",
      "\n",
      "23\n",
      "\n",
      "\fModel\n",
      "\n",
      "Size\n",
      "\n",
      "# tokens\n",
      "per image\n",
      "\n",
      "7B-14B models\n",
      "LLaVA-NeXT\n",
      "DeepSeek-VL\n",
      "MM1-Chat\n",
      "Idefics2\n",
      "Idefics2\n",
      "\n",
      "≥30B models\n",
      "\n",
      "13B\n",
      "7B\n",
      "7B\n",
      "8B\n",
      "8B\n",
      "\n",
      "Mini-Gemini-HD 34B\n",
      "30B\n",
      "34B\n",
      "\n",
      "MM1-Chat\n",
      "LLaVA-NeXT\n",
      "\n",
      "Proprietary\n",
      "Gemini 1.0 Pro\n",
      "Claude 3 Haiku\n",
      "Claude 3 Sonnet\n",
      "Gemini 1.5 Pro\n",
      "\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "\n",
      "2880\n",
      "576\n",
      "720\n",
      "64\n",
      "320\n",
      "\n",
      "2880\n",
      "720\n",
      "2880\n",
      "\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "\n",
      "U\n",
      "M\n",
      "M\n",
      "M\n",
      "\n",
      "36.2/-\n",
      "36.6/-\n",
      "37.0/35.6\n",
      "43.5/37.9\n",
      "43.0/37.7\n",
      "\n",
      "48.0/44.9\n",
      "44.7/40.3\n",
      "51.1/44.7\n",
      "\n",
      "47.9/-\n",
      "50.2/-\n",
      "53.1/-\n",
      "58.5/-\n",
      "\n",
      "a\n",
      "t\n",
      "s\n",
      "i\n",
      "V\n",
      "h\n",
      "t\n",
      "a\n",
      "M\n",
      "\n",
      "35.3\n",
      "36.1\n",
      "35.9\n",
      "51.6\n",
      "51.4\n",
      "\n",
      "43.3\n",
      "39.4\n",
      "46.5\n",
      "\n",
      "45.2\n",
      "46.4\n",
      "47.9\n",
      "52.1\n",
      "\n",
      "A\n",
      "Q\n",
      "V\n",
      "t\n",
      "x\n",
      "e\n",
      "T\n",
      "\n",
      "67.1\n",
      "64.4\n",
      "72.8\n",
      "70.4\n",
      "73.0\n",
      "\n",
      "74.1\n",
      "73.5\n",
      "69.5\n",
      "\n",
      "74.6\n",
      "-\n",
      "-\n",
      "73.5\n",
      "\n",
      "h\n",
      "c\n",
      "n\n",
      "e\n",
      "B\n",
      "M\n",
      "M\n",
      "\n",
      "70.0\n",
      "73.2\n",
      "72.3\n",
      "76.8\n",
      "76.7\n",
      "\n",
      "80.6\n",
      "75.1\n",
      "79.3\n",
      "\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "\n",
      "A\n",
      "Q\n",
      "V\n",
      "c\n",
      "o\n",
      "D\n",
      "\n",
      "-\n",
      "49.6\n",
      "-\n",
      "67.3\n",
      "74.0\n",
      "\n",
      "-\n",
      "-\n",
      "-\n",
      "\n",
      "88.1\n",
      "88.8\n",
      "89.5\n",
      "86.5\n",
      "\n",
      "2\n",
      "v\n",
      "A\n",
      "Q\n",
      "V\n",
      "\n",
      "82.8\n",
      "-\n",
      "82.8\n",
      "80.8\n",
      "81.2\n",
      "\n",
      "-\n",
      "83.7\n",
      "83.7\n",
      "\n",
      "71.2\n",
      "-\n",
      "-\n",
      "73.2\n",
      "\n",
      "Table 15: Performance of Idefics2 against state-of-the-art VLMs across different sizes. The eval-\n",
      "uations are done in zero shot. Idefics2 with 64 or 320 tokens per image only differs by the image\n",
      "splitting.\n",
      "(Benchmark, Split, Metric): (MMMU, val/test, MMMU score), (MathVista, testmini/test, MMMU\n",
      "score), (TextVQA, val, VQA acc.), (MMBench, test, accuracy), (DocVQA, test, ANLS score), (VQAv2,\n",
      "testdev, VQA acc.).\n",
      "\n",
      "Figure 5: Idefics2-chatty finds the requested information in the resume, and organizes it in JSON\n",
      "format.\n",
      "\n",
      "While the model typically refrains from responding to offensive inputs, we observe that through\n",
      "repeated trials or guided interactions, it tends to hastily form judgments in situations necessitating\n",
      "\n",
      "24\n",
      "\n",
      "PromptExtract the name, email, current job, and education. Output a JSON format.Idefics2 output{  \"name\": \"Yann Karpathy\",  \"email\": \"yann.karpathy@gmail.com\",  \"current_job\": \"Machine Learning Engineer, Hugging Face\",  \"education\": [    {      \"year\": \"2012 - 2018\",      \"degree\": \"Ph.D. in Computer Science\",      \"university\": \"Stanford University\"    },    {      \"year\": \"2010 - 2012\",      \"degree\": \"Master of Science in Artificial Intelligence\",      \"university\": \"Massachusetts Institute of Technology\"    },    {      \"year\": \"2006 - 2010\",      \"degree\": \"Bachelor of Science in Computer Science\",      \"university\": \"University of Toronto\"    }  ]}\fFigure 6: Idefics2-chatty describes an AI-generated image.\n",
      "\n",
      "Figure 7: Idefics2-chatty answers a question on a scientific diagram.\n",
      "\n",
      "nuanced contextual understanding, often perpetuating harmful stereotypes. Noteworthy instances\n",
      "include:\n",
      "\n",
      "• Speculating or passing judgments, or perpetuating historical disparities on individuals’\n",
      "professions, social status, or insurance eligibility based solely on visual cues (e.g., age, attire,\n",
      "gender, facial expressions).\n",
      "\n",
      "• Generating content that promotes online harassment or offensive memes reinforcing harmful\n",
      "\n",
      "associations from a portrait, or from a benign image.\n",
      "\n",
      "• Assuming emotional states or mental conditions based on outward appearances.\n",
      "\n",
      "• Evaluating individuals’ attractiveness solely based on their visual appearance.\n",
      "\n",
      "Additionally, we identify behaviors that increase security risks that already exist:\n",
      "\n",
      "• Successfully solving CAPTCHAs featuring distorted text within images.\n",
      "\n",
      "• Developing phishing schemes from screenshots of legitimate websites to deceive users into\n",
      "\n",
      "divulging their credentials.\n",
      "\n",
      "• Crafting step-by-step guides on constructing small-scale explosives using readily available\n",
      "chemicals from common supermarkets or manipulating firearms to do maximum damage.\n",
      "\n",
      "It’s important to note that these security concerns are currently limited by the model’s occasional\n",
      "inability to accurately read text within images.\n",
      "\n",
      "25\n",
      "\n",
      "PromptDescribe the imageIdefics2 outputThe image depicts a surreal scene where three robot soldiers are standing on a street in front of the Eiffel Tower in Paris. Each robot soldier is equipped with a large cannon that resembles a giant bread roll. The robots are facing towards the right side of the image, and they appear to be ready for action. The sky above them is filled with clouds, and there's a red light visible in the distance. The overall atmosphere of the image is quite unusual, combining elements of technology and architecture in an unexpected way.PromptWhat happens to fish if pelicans increase?Idefics2 outputIf pelicans increase, they may consume more fish, which could lead to a decrease in the fish population or an imbalance in the ecosystem. This could potentially affect other species that rely on fish for food, such as seals, dolphins, and humans who fish for consumption.\fWe emphasize that the model would often encourage the user to exercise caution about the model’s\n",
      "generation or flag how problematic the initial query can be in the first place. For instance, when\n",
      "insistently prompted to write a racist comment, the model would answer that query before pointing\n",
      "out \"This type of stereotyping and dehumanization has been used throughout history to justify\n",
      "discrimination and oppression against people of color. By making light of such a serious issue, this\n",
      "meme perpetuates harmful stereotypes and contributes to the ongoing struggle for racial equality\n",
      "and social justice.\".\n",
      "\n",
      "However, certain formulations can circumvent (i.e. \"jailbreak\") these cautionary prompts, empha-\n",
      "sizing the need for critical thinking and discretion when engaging with the model’s outputs. While\n",
      "jail-breaking text LLMs is an active research area, jail-breaking vision-language models have re-\n",
      "cently emerged as a new challenge as vision-language models become more capable and prominent\n",
      "(Shayegani et al., 2024). The addition of the vision modality not only introduces new avenues\n",
      "for injecting malicious prompts but also raises questions about the interaction between vision and\n",
      "language vulnerabilities.\n",
      "\n",
      "26\n",
      "\n",
      "\f\n"
     ]
    }
   ],
   "source": [
    "from pdfminer.layout import LAParams, LTTextBox\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfinterp import PDFResourceManager\n",
    "from pdfminer.pdfinterp import PDFPageInterpreter\n",
    "from pdfminer.converter import PDFPageAggregator\n",
    "from pdfminer.converter import TextConverter\n",
    "import io\n",
    "\n",
    "resource_manager = PDFResourceManager()\n",
    "fake_file_handle = io.StringIO()\n",
    "converter = TextConverter(resource_manager, fake_file_handle, laparams=LAParams())\n",
    "page_interpreter = PDFPageInterpreter(resource_manager, converter)\n",
    "\n",
    "with open(filename, 'rb') as fh:\n",
    "\n",
    "    for page in PDFPage.get_pages(fh,\n",
    "                                  caching=True,\n",
    "                                  check_extractable=True):\n",
    "        page_interpreter.process_page(page)\n",
    "\n",
    "    text = fake_file_handle.getvalue()\n",
    "\n",
    "# close open handles\n",
    "converter.close()\n",
    "fake_file_handle.close()\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
