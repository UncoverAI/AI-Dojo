{
    "seed": 42,
    "data_file_path": "/llm/data/sascha_train.jsonl",
    "model_name_or_path": "meta-llama/Meta-Llama-3-8B-Instruct",
    "max_seq_length": 128,
    "num_train_epochs": 1,
    "logging_steps": 5,
    "log_level": "info",
    "logging_strategy": "steps",
    "bf16":true,
    "learning_rate": 2.0e-05,
    "lr_scheduler_type": "cosine",
    "output_dir": "/llm/model",
    "per_device_train_batch_size": 2,
    "per_device_eval_batch_size": 2,
    "gradient_accumulation_steps": 2,
    "gradient_checkpointing":true,
    "use_reentrant":true,
    "dataset_text_field": "text",
    "use_flash_attn":true,
    "use_peft_lora":true,
    "lora_r": 8,
    "lora_alpha": 16,
    "lora_dropout": 0.1,
    "lora_target_modules": "all-linear",
    "use_4bit_quantization":true,
    "use_nested_quant":true,
    "bnb_4bit_compute_dtype": "bfloat16",
    "bnb_4bit_quant_storage_dtype": "bfloat16"
}