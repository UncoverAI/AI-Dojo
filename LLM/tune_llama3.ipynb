{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q -U trl numpy torch peft transformers  datasets bitsandbytes wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import wandb\n",
    "from datasets import load_dataset\n",
    "from scipy.special import softmax\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "from sklearn.metrics import accuracy_score, f1_score, log_loss, confusion_matrix\n",
    "from transformers import set_seed, TrainingArguments, Trainer, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Quantized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging face login\n",
    "token='YOUR-API-KEY'\n",
    "\n",
    "#quantization configurations - so you quantize the model while inferencing\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_qunat_type = \"nf4\",\n",
    "    bnb_4bit_compute_dtype = torch.float16,\n",
    ")\n",
    "\n",
    "model_name = 'meta-llama/Meta-Llama-3-8B-Instruct'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.bos_token\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2, # Change according to your case, it is hate / non-hate in our work.\n",
    "    quantization_config=bnb_config, \n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    token=token\n",
    "    )\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Initial trainable parameters of our model.\n",
    "def count_trainable_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "num_params = count_trainable_params(model)\n",
    "formatted_num_params = \"{:,}\".format(num_params)\n",
    "print(f\"Number of trainable parameters: {formatted_num_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quantization configurations - so you quantize the model while inferencing\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_qunat_type = \"nf4\",\n",
    "    bnb_4bit_compute_dtype = torch.float16,\n",
    ")\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Initial trainable parameters of our model.\n",
    "def count_trainable_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "num_params = count_trainable_params(model)\n",
    "formatted_num_params = \"{:,}\".format(num_params)\n",
    "print(f\"Number of trainable parameters: {formatted_num_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "print(tokenizer.decode(response, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('csv', data_files={'train': ['path-to-your-train-dataset'],\n",
    "                                          'test':['path-to-your-test-dataset']})\n",
    "def tokenize(examples):\n",
    "        return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "train_dataset = dataset['train'].map(tokenize, batched=True)\n",
    "test_dataset = dataset['test'].map(tokenize, batched=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure LoRA Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "==========================================================================================\n",
    "After extensive trainings and experience my suggestions are,\n",
    "use only (k, q, v, o) projections or else it may overfitt,\n",
    "For DoRA as paper suggests lower rank may be optimal my choice is (r=8, alpha=8 or 16)\n",
    "If you increase rank (r=16) and alpha(16<) try dropout (0.2).\n",
    "==========================================================================================\n",
    "Lastly, it all boils down to your specific use case. \n",
    "Experiment with your own choices to grasp your own hyperparameters for your task.\n",
    "==========================================================================================\n",
    "'''\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=64,\n",
    "        lora_dropout=0.2,\n",
    "        bias=\"none\",\n",
    "        task_type='SEQ_CLS',\n",
    "        use_dora=True # Make this False if you wanna use only LoRA\n",
    "        target_modules=[\n",
    "    \"up_proj\",\n",
    "    \"o_proj\",\n",
    "    \"v_proj\",\n",
    "    \"gate_proj\",\n",
    "    \"q_proj\",\n",
    "    \"down_proj\",\n",
    "    \"k_proj\"\n",
    "  ]\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the parameters and hyper-parameters as per your use case.\n",
    "epochs = 10\n",
    "batch_size = 5\n",
    "gradient_accumulation_steps = 4\n",
    "\n",
    "# output dir \n",
    "model_version = \"openchat_3.5_QLoRA\"\n",
    "model_dir = f\"{model_version}\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "        run_name=model_version,\n",
    "        logging_dir=f\"{model_dir}/logs\",\n",
    "        output_dir=model_dir,\n",
    "        logging_steps=100,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        num_train_epochs=epochs,\n",
    "        lr_scheduler_type=\"constant\",\n",
    "        save_strategy=\"epoch\",\n",
    "        fp16=True,\n",
    "\n",
    "    )\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "# The parameters after appling LoRA\n",
    "num_params = count_trainable_params(model)\n",
    "formatted_num_params = \"{:,}\".format(num_params)\n",
    "print(f\"Number of trainable parameters: {formatted_num_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# designing computing metrics as per our use case. (F1-Macro is essential and log-loss is optional)\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p.predictions, p.label_ids\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    macro_f1 = f1_score(labels, predictions, average='macro')\n",
    "\n",
    "    return {\"accuracy\": accuracy, \"macro_f1\": macro_f1}\n",
    "\n",
    "# configure Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Store progress and track with wandb\n",
    "wandb.init(\n",
    "project=\"HOLD-Final\", # Name of the dir you wanted to store this run\n",
    "name=model_version # Run name\n",
    ")\n",
    "\n",
    "# start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save trained model\n",
    "trainer.save_model(f'{model_dir}/model')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
